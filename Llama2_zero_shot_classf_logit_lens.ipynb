{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12320c-181b-4c3e-bf87-f31671978722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ.get('SLURM_JOB_NODELIST'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9c89f-2d80-49ef-8e41-c2c828aad086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, utils\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from transformer_lens.utils import get_corner, gelu_new, tokenize_and_concatenate\n",
    "import tqdm.auto as tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from transformer_lens.utils import get_act_name\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472268f-0a81-44b5-b77a-af2b37d37e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Move the model to the GPU for faster processing\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed381fc-11f5-462e-9cda-da757231a78a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a7b11-1e01-4598-8854-73148ba5df4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Emotion\n",
    "\n",
    "df_train = pd.read_csv('~/transformers_play/emotional_dataset/training.csv')\n",
    "print(df_train)\n",
    "\n",
    "df_test = pd.read_csv('~/transformers_play/emotional_dataset/test.csv')\n",
    "print(df_test)\n",
    "\n",
    "df_val = pd.read_csv('~/transformers_play/emotional_dataset/validation.csv')\n",
    "print(df_val)\n",
    "\n",
    "#IMDB sentiment\n",
    "\n",
    "df_senti = pd.read_csv('~/transformers_play/emotional_dataset/IMDB_Dataset.csv')\n",
    "print(df_senti.columns)\n",
    "print(df_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b19c19-79c8-4aaa-a0e1-4e586e043088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if df_train['text'].isna().sum() > 0:\n",
    "    print(\"Warning: Some text entries are missing.\")\n",
    "    df_train = df_train.dropna(subset=['text'])\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd1779-5eac-4a1b-8b60-6b12a6e34abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_labels = {\n",
    "    'sadness': 0,\n",
    "    'joy': 1,\n",
    "    'love': 2,\n",
    "    'anger': 3,\n",
    "    'fear': 4,\n",
    "    'surprise': 5\n",
    "}\n",
    "\n",
    "\n",
    "id_to_emotion = {id: label for label, id in emotion_labels.items()}\n",
    "\n",
    "# Apply the inverse mapping to a new column for easy viewing\n",
    "df_train['emotion_name'],df_test['emotion_name']  = df_train['label'].map(id_to_emotion), df_test['label'].map(id_to_emotion)\n",
    "\n",
    "# Display the DataFrame to see both the integer label and the emotion name\n",
    "df_train, df_test\n",
    "\n",
    "# Filter for the first 20 rows\n",
    "df_test_short = df_test[:50]\n",
    "\n",
    "# Create a mapping from ID to name for printing\n",
    "emotion_names = {v: k for k, v in emotion_labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef11379-9503-4eb3-a845-2c68e5ad6c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "senti_labels = {\n",
    "    'positive': 0,\n",
    "    'negative': 1 }\n",
    "\n",
    "\n",
    "id_to_senti = {id: sentiment for sentiment, id in senti_labels.items()}\n",
    "\n",
    "\n",
    "# Create a mapping from ID to name for printing\n",
    "\n",
    "senti_names = {v: k for k, v in senti_labels.items()}\n",
    "\n",
    "\n",
    "X = df_senti['review']\n",
    "y = df_senti['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1012c127-f262-45e8-a33c-eb5c64d0c5e7",
   "metadata": {},
   "source": [
    "# Initialising Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f35abb3-a233-4c6c-b7cb-b159d3578740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Now load the model\n",
    "try:\n",
    "    model_llama = HookedTransformer.from_pretrained(model_name, fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "    print(f\"Loaded {model_name} successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5380bc1c-51c2-446f-b83a-3c44cc21d63d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "hf_token = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "   \n",
    "    login(token=hf_token)\n",
    "    \n",
    "    \n",
    "    model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "    try:\n",
    "        model_llama = HookedTransformer.from_pretrained(model_name, fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "        print(f\"Loaded {model_name} successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Hugging Face token not found. Please set the 'HUGGING_FACE_HUB_TOKEN' environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41daad4d-9210-4ad2-8f68-c240d1d8ee1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = model_llama.tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc15bf9b-48de-405b-b9cc-0ab33fa55e92",
   "metadata": {
    "tags": []
   },
   "source": [
    "single_prompt = \"The cinema was slow but the acting was excellent. What is the emotion of this text? Choose from sadness, joy, love, anger, fear, surprise. The emotion is:\"\n",
    "\n",
    "tokens = model_llama.to_tokens(single_prompt)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "token_str = model_llama.to_str_tokens(single_prompt)\n",
    "\n",
    "print(token_str)\n",
    "\n",
    "print(tokens.shape)\n",
    "\n",
    "\n",
    "tokens = tokens.cuda()\n",
    "\n",
    "logits, cache = model_llama.run_with_cache(tokens)\n",
    "\n",
    "print(f\"logits shape\",logits.shape)\n",
    "\n",
    "print(logits[:1,:5,:5])\n",
    "\n",
    "# Convert logits to a distribution with softmax\n",
    "\n",
    "probs = logits.log_softmax(dim=-1)\n",
    "print(probs.shape)\n",
    "probs[:1,:5,:5].max()\n",
    "probs.max()\n",
    "\n",
    "\n",
    "last_sequence_in_batch = logits[0,-1,:]\n",
    "last_sequence_in_batch.shape\n",
    "\n",
    "predict_token = last_sequence_in_batch.argmax(dim=-1)\n",
    "print(predict_token)\n",
    "\n",
    "predict_token.item()\n",
    "\n",
    "model_llama.tokenizer.decode(predict_token.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b98a4-ade5-4d7b-aafe-c586adcc9b52",
   "metadata": {},
   "source": [
    "## Constraint based prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ac230-54c6-4aeb-b3b3-53dc0d5ce924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Zero-Shot vs. Constraint-Based Prompting Comparison ---\")\n",
    "print(\"This will process the same text with two different prompts to compare the results.\")\n",
    "\n",
    "true_emotions = []\n",
    "predicted_emotions = []\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for i, row in df_test[:].iterrows():\n",
    "    \n",
    "    text = row['text']\n",
    "    true_label_id = row['label']\n",
    "    true_label_name = row['emotion_name'] #emotion_names.get(true_label_id, \"unknown\")\n",
    "\n",
    "    # --- Case 1: Zero-Shot Prompting ---\n",
    "    \n",
    "    zero_shot_prompt = f\"{text}\\nWhat is the emotion of this text? Choose from sadness, joy, love, anger, fear, surprise.\"\n",
    "\n",
    "    # Tokenize and move to GPU\n",
    "    \n",
    "    inputs_zero = tokenizer(zero_shot_prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs_zero = {k: v.to('cuda') for k, v in inputs_zero.items()}\n",
    "    \n",
    "    # Generate the next 10 tokens\n",
    "    \n",
    "    output_tokens_zero = model_llama.generate(\n",
    "        inputs_zero['input_ids'],\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    # Isolate the newly generated tokens for the zero-shot case\n",
    "    \n",
    "    new_tokens_zero = tokenizer.decode(output_tokens_zero[0][inputs_zero['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the predicted emotion from the generated text\n",
    "    \n",
    "    predicted_zero_shot = \"unknown\"\n",
    "    \n",
    "    for emotion_name in emotion_labels.keys():\n",
    "        \n",
    "        if emotion_name in new_tokens_zero.lower():\n",
    "            \n",
    "            predicted_zero_shot = emotion_name\n",
    "            \n",
    "            break\n",
    "\n",
    "    # --- Case 2: Constraint-Based Prompting ---\n",
    "    \n",
    "    constraint_prompt = f\"{text}\\nWhat is the single emotion of this text? You must choose one and only one from the following list: sadness, joy, love, anger, fear, surprise. The emotion is:\"\n",
    "\n",
    "    # Tokenize and move to GPU\n",
    "    \n",
    "    inputs_constraint = tokenizer(constraint_prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs_constraint = {k: v.to('cuda') for k, v in inputs_constraint.items()}\n",
    "    \n",
    "    # Generate the next 10 tokens\n",
    "    \n",
    "    output_tokens_constraint = model_llama.generate(\n",
    "        inputs_constraint['input_ids'],\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    # Isolate the newly generated tokens for the constraint-based case\n",
    "    \n",
    "    predicted_new_tokens_constraint = tokenizer.decode(output_tokens_constraint[0][inputs_constraint['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the predicted emotion from the generated text\n",
    "    \n",
    "    predicted_constraint = \"unknown\"\n",
    "    \n",
    "    cleaned_output = predicted_new_tokens_constraint.strip().lower()\n",
    "    \n",
    "    for word in cleaned_output.replace(\"a. \", \"\").replace(\"b. \", \"\").replace(\"\\n\", \" \").split():\n",
    "        \n",
    "        clean_word = word.strip(\".,:;\").lower()\n",
    "        \n",
    "        if clean_word in emotion_labels.keys():\n",
    "            \n",
    "            predicted_constraint = clean_word\n",
    "            \n",
    "            break\n",
    "     \n",
    "    true_emotions.append(true_label_name)\n",
    "    predicted_emotions.append(predicted_constraint)#predicted_emotions.append(cleaned_output)\n",
    "    \n",
    "    status = 1 if true_label_name == predicted_constraint else 0\n",
    "\n",
    "\n",
    "    results_list.append({\n",
    "            'text': text,\n",
    "            'emotion': true_label_name,\n",
    "            'constrained prompt': f\"{text}\\nWhat the single emotion of this text? You must choose one and only one from the following list: sadness, joy, love, anger, fear, surprise. The emotion is:\",\n",
    "            'prompt response' : cleaned_output,\n",
    "            'predicted emotion': predicted_constraint,\n",
    "            'Output status': status\n",
    "            \n",
    "        })        \n",
    "\n",
    "    # --- Print Comparison Results ---\n",
    "    \n",
    "    \n",
    "    print(f\"\\n--- Text {i+1} ---\")\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"True Emotion: {true_label_name}\")\n",
    "    \n",
    "    print(\"\\n--- Zero-Shot Prompting ---\")\n",
    "    \n",
    "    print(f\"Prompt: {zero_shot_prompt}\")\n",
    "    print(f\"Generated Output: {new_tokens_zero.strip()}\")\n",
    "    print(f\"Predicted Emotion: {predicted_zero_shot}\")\n",
    "    \n",
    "    print(\"\\n--- Constraint-Based Prompting ---\")\n",
    "    \n",
    "    print(f\"Prompt: {constraint_prompt}\")\n",
    "    print(f\"Generated Output: {predicted_new_tokens_constraint.strip()}\")\n",
    "    print(f\"Predicted Emotion: {predicted_constraint}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819def9b-be23-4637-a9be-627dd6236ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_constrained_prompt = pd.DataFrame(results_list)\n",
    "df_constrained_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d412265d-fa50-4723-b4e6-f475ea9f67e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df_constrained_prompt.to_csv('constrained_prompt_results_llama2.csv', index=False)\n",
    "\n",
    "print(\"DataFrame saved successfully to 'constrained_prompt_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa01a3e-a8a7-408e-89c1-c0f121345944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "   \n",
    "    df_constrained_prompt_loaded = pd.read_csv('constrained_prompt_results_llama2.csv')\n",
    "    \n",
    "    print(\"DataFrame loaded successfully.\")\n",
    "    print( df_constrained_prompt_loaded.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"The file 'constrained_prompt_results.csv' was not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d847a9-63a5-4c2a-8b4f-a4b4923db864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Generating Performance Histogram ---\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "# The bins are set to center the bars at 0 and 1\n",
    "\n",
    "df_constrained_prompt_loaded['Output status'].hist( align='mid', rwidth=0.8)\n",
    "\n",
    "plt.xticks([0, 1], ['Hallucinated', 'Non hallucinated'])\n",
    "plt.title('Prediction Status: Hallucinated(0) vs. Non hallucinated (1)')\n",
    "plt.xlabel('Prediction Status')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe038e-35f3-48af-8293-acabfaa77c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate and print the overall accuracy\n",
    "\n",
    "true_emotions = df_constrained_prompt_loaded['emotion']\n",
    "predicted_emotions = df_constrained_prompt_loaded['predicted emotion']\n",
    "\n",
    "\n",
    "try:\n",
    "    accuracy = accuracy_score(true_emotions, predicted_emotions)\n",
    "    print(\"\\n--- Overall Accuracy ---\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not calculate accuracy: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a8f97-07ba-4f2e-95cc-6b7d75d1a41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and display a confusion matrix to visualize errors\n",
    "\n",
    "print(\"\\n--- Generating Confusion Matrix ---\")\n",
    "cm = confusion_matrix(true_emotions, predicted_emotions, labels=list(set(true_emotions)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(set(true_emotions)))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994832cc-d92e-4807-b542-b535353490bc",
   "metadata": {},
   "source": [
    "## Mechanistic Interpretibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd183f-8c4a-41b9-9865-3de7fd40916d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "hallu_df = df_constrained_prompt_loaded[df_constrained_prompt_loaded['Output status'] == 0].copy()\n",
    "\n",
    "non_hallu_df = df_constrained_prompt_loaded[df_constrained_prompt_loaded['Output status'] == 1].copy()\n",
    "\n",
    "hallu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13cb55-da4c-4d99-b9d1-e1b0b78f4880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hallucination_patterns = hallu_df.groupby(['emotion', 'predicted emotion']).size().reset_index(name='count')\n",
    "    \n",
    "\n",
    "hallucination_patterns = hallucination_patterns.sort_values(by='count', ascending=False)\n",
    "    \n",
    "\n",
    "hallucination_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9615929c-5c7e-4216-8da0-04056b49f686",
   "metadata": {},
   "source": [
    "## Joy Hallucinations (True Emotion: Joy)\n",
    "joy-sadness: hallu_set1_df\n",
    "\n",
    "joy-love: hallu_set2_df\n",
    "\n",
    "joy-fear: hallu_set3_df\n",
    "\n",
    "joy-surprise: hallu_set4_df\n",
    "\n",
    "joy-anger: hallu_set5_df\n",
    "\n",
    "## Anger Hallucinations (True Emotion: Anger)\n",
    "anger-sadness: hallu_set6_df\n",
    "\n",
    "anger-joy: hallu_set7_df\n",
    "\n",
    "anger-fear: hallu_set8_df\n",
    "\n",
    "anger-love: hallu_set9_df\n",
    "\n",
    "anger-surprise: hallu_set10_df\n",
    "\n",
    "## Sadness Hallucinations (True Emotion: Sadness)\n",
    "sadness-fear: hallu_set11_df\n",
    "\n",
    "sadness-love: hallu_set12_df\n",
    "\n",
    "sadness-surprise: hallu_set13_df\n",
    "\n",
    "sadness-joy: hallu_set14_df\n",
    "\n",
    "sadness-anger: hallu_set15_df\n",
    "\n",
    "## Fear Hallucinations (True Emotion: Fear)\n",
    "fear-sadness: hallu_set16_df\n",
    "\n",
    "fear-love: hallu_set17_df\n",
    "\n",
    "fear-anger: hallu_set18_df\n",
    "\n",
    "fear-surprise: hallu_set19_df\n",
    "\n",
    "fear-joy: hallu_set20_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40583f21-b1d3-4a16-8585-45d473c0a758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "hallu_set1_df = hallu_df[(hallu_df['emotion'] == 'joy') & (hallu_df['predicted emotion'] == 'sadness')]\n",
    "\n",
    "\n",
    "hallu_set2_df = hallu_df[(hallu_df['emotion'] == 'joy') & (hallu_df['predicted emotion'] == 'love')]\n",
    "\n",
    "\n",
    "hallu_set3_df = hallu_df[(hallu_df['emotion'] == 'joy') & (hallu_df['predicted emotion'] == 'fear')]\n",
    "\n",
    "\n",
    "hallu_set4_df = hallu_df[(hallu_df['emotion'] == 'joy') & (hallu_df['predicted emotion'] == 'surprise')]\n",
    "\n",
    "\n",
    "hallu_set5_df = hallu_df[(hallu_df['emotion'] == 'joy') & (hallu_df['predicted emotion'] == 'anger')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hallu_set6_df = hallu_df[(hallu_df['emotion'] == 'anger') & (hallu_df['predicted emotion'] == 'sadness')]\n",
    "\n",
    "\n",
    "hallu_set7_df = hallu_df[(hallu_df['emotion'] == 'anger') & (hallu_df['predicted emotion'] == 'joy')]\n",
    "\n",
    "\n",
    "hallu_set8_df = hallu_df[(hallu_df['emotion'] == 'anger') & (hallu_df['predicted emotion'] == 'fear')]\n",
    "\n",
    "\n",
    "hallu_set9_df = hallu_df[(hallu_df['emotion'] == 'anger') & (hallu_df['predicted emotion'] == 'love')]\n",
    "\n",
    "\n",
    "hallu_set10_df = hallu_df[(hallu_df['emotion'] == 'anger') & (hallu_df['predicted emotion'] == 'surprise')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hallu_set11_df = hallu_df[(hallu_df['emotion'] == 'sadness') & (hallu_df['predicted emotion'] == 'fear')]\n",
    "\n",
    "\n",
    "hallu_set12_df = hallu_df[(hallu_df['emotion'] == 'sadness') & (hallu_df['predicted emotion'] == 'love')]\n",
    "\n",
    "\n",
    "hallu_set13_df = hallu_df[(hallu_df['emotion'] == 'sadness') & (hallu_df['predicted emotion'] == 'surprise')]\n",
    "\n",
    "\n",
    "hallu_set14_df = hallu_df[(hallu_df['emotion'] == 'sadness') & (hallu_df['predicted emotion'] == 'joy')]\n",
    "\n",
    "\n",
    "hallu_set15_df = hallu_df[(hallu_df['emotion'] == 'sadness') & (hallu_df['predicted emotion'] == 'anger')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hallu_set16_df = hallu_df[(hallu_df['emotion'] == 'fear') & (hallu_df['predicted emotion'] == 'sadness')]\n",
    "\n",
    "\n",
    "hallu_set17_df = hallu_df[(hallu_df['emotion'] == 'fear') & (hallu_df['predicted emotion'] == 'love')]\n",
    "\n",
    "\n",
    "hallu_set18_df = hallu_df[(hallu_df['emotion'] == 'fear') & (hallu_df['predicted emotion'] == 'anger')]\n",
    "\n",
    "\n",
    "hallu_set19_df = hallu_df[(hallu_df['emotion'] == 'fear') & (hallu_df['predicted emotion'] == 'surprise')]\n",
    "\n",
    "\n",
    "hallu_set20_df = hallu_df[(hallu_df['emotion'] == 'fear') & (hallu_df['predicted emotion'] == 'joy')]\n",
    "\n",
    "\n",
    "\n",
    "hallu_set_unkown_df = hallu_df[(hallu_df['emotion'] == 'joy') & (hallu_df['predicted emotion'] == 'unknown')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac0bc8-c0e0-45d9-a575-bb928f1356b3",
   "metadata": {},
   "source": [
    "## Generate logits and cache for all the hallu prompt samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dee3fe-9eda-4fca-9152-351ad13fdefa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "def generate_and_save_caches(model, tokenizer, hallu_dfs: List[pd.DataFrame], save_dir: str = \"cached_data\"):\n",
    "    \"\"\"\n",
    "    Runs the model on all prompts and saves the original logits and caches to disk.\n",
    "    \n",
    "    This is an memory-efficient method for large datasets.\n",
    "    \n",
    "    Args:\n",
    "        model: The loaded TransformerLens model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        hallu_dfs (List[pd.DataFrame]): A list of DataFrames, each containing hallucinating prompts.\n",
    "        save_dir (str): The directory where the caches will be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    all_prompts_df = pd.concat(hallu_dfs, ignore_index=True)\n",
    "\n",
    "    for index, row in all_prompts_df.iterrows():\n",
    "        prompt = row['constrained prompt']\n",
    "        \n",
    "        \n",
    "        cache_filepath = os.path.join(save_dir, f\"cache_{index}.pt\")\n",
    "        logits_filepath = os.path.join(save_dir, f\"logits_{index}.pt\")\n",
    "        \n",
    "        if os.path.exists(cache_filepath) and os.path.exists(logits_filepath):\n",
    "            print(f\"Skipping index {index}, files already exist.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing prompt {index}...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                logits, cache = model.run_with_cache(prompt)\n",
    "                \n",
    "                \n",
    "                torch.save(logits, logits_filepath)\n",
    "                torch.save(cache, cache_filepath)\n",
    "                \n",
    "               \n",
    "                del logits, cache\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                if \"CUDA out of memory\" in str(e):\n",
    "                    print(f\"OutOfMemoryError for prompt: {prompt}. Skipping...\")\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "    print(\"All caches have been generated and saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551518ed-6303-45a5-8cde-3086ed7ee2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#all_hallu_sample_dfs = [hallu_set1_df[:5]]\n",
    "\n",
    "generate_and_save_caches(model_llama, tokenizer, [hallu_set1_df[:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe1d52-e58c-4a86-9625-3a6f2e698d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def load_saved_caches(save_dir: str = \"cached_data\") -> Dict:\n",
    "    loaded_data = {}\n",
    "    cache_files = [f for f in os.listdir(save_dir) if f.startswith('cache_') and f.endswith('.pt')]\n",
    "    \n",
    "    for cache_file in cache_files:\n",
    "        index_str = cache_file.split('_')[1].split('.')[0]\n",
    "        index = int(index_str)\n",
    "        \n",
    "        logits_filepath = os.path.join(save_dir, f\"logits_{index}.pt\")\n",
    "        cache_filepath = os.path.join(save_dir, cache_file)\n",
    "        \n",
    "        try:\n",
    "            logits = torch.load(logits_filepath)\n",
    "            \n",
    "            # Change this line\n",
    "            cache = torch.load(cache_filepath, weights_only=False)\n",
    "            \n",
    "            loaded_data[index] = {'logits': logits, 'cache': cache}\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Missing logits file for index {index}. Skipping...\")\n",
    "            \n",
    "    print(f\"Loaded data for {len(loaded_data)} prompts.\")\n",
    "    return loaded_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a0bec-f89f-4e4b-8abe-d0a528a77ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506d487-677f-4a46-973f-9b4769262a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a97f13-7ce8-4d35-bfc9-8109a5ea4eed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the model on the text and save all the layer activations to a cache.\n",
    "\n",
    "text_to_analyze = hallu_df['text'].iloc[0]\n",
    "    \n",
    "# run_with_cache returns the final logits and a cache object.\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    logits, cache = model_llama.run_with_cache(text_to_analyze)\n",
    "\n",
    "   \n",
    "print(\"\\n--- Inspecting Activations (Cache) ---\")\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(model_llama.cfg.n_layers):\n",
    "    \n",
    "        \n",
    "    attention_out = cache[\"pattern\", i]\n",
    "       \n",
    "    print(f\"Layer {i} Attention Output Shape: {attention_out.shape}\") # The shape is (batch, position, head, head_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290de91-b7b3-4df4-a3bf-039dc33601ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,keys in enumerate(cache.keys()):\n",
    "    print(i,keys)\n",
    "    if i==20:\n",
    "        break\n",
    "   \n",
    "# output of the MLP block.\n",
    "layer = 1\n",
    "\n",
    "mlp_out = cache[f'blocks.{layer}.mlp.hook_post']\n",
    "\n",
    "print(mlp_out.shape)\n",
    "\n",
    "print(f\"Layer {layer} MLP Output Shape: {mlp_out.shape}\")\n",
    "print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df275e64-c36c-4677-9656-a769272f6469",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708ea57-4bfe-44c6-b5a5-32028c11ccc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(hallu_df['emotion'].iloc[6])\n",
    "print(hallu_df['predicted emotion'].iloc[6])\n",
    "print(hallu_df['constrained prompt'].iloc[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa1efa-45e9-44c6-bda7-cd2d7f081b17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_token_ids(tokenizer, text):\n",
    "    \"\"\"\n",
    "    Gets the token IDs for a text string, handling spaces.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        encoded_with_space = tokenizer.encode(f\" {text}\", add_special_tokens=False)\n",
    "        if encoded_with_space:\n",
    "            return encoded_with_space\n",
    "        \n",
    "        \n",
    "        encoded_without_space = tokenizer.encode(text, add_special_tokens=False)\n",
    "        if encoded_without_space:\n",
    "            return encoded_without_space\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error for '{text}': {e}\", file=sys.stderr)\n",
    "        \n",
    "    raise ValueError(f\"Could not find any tokens for '{text}'.\")\n",
    "\n",
    "# --- Rank Calculation Function ---\n",
    "\n",
    "def get_rank(logits, token_ids):\n",
    "    \"\"\"\n",
    "    Calculates the rank of the given tokens in the logits tensor.\n",
    "    Rank is 1-based, where 1 is the highest logit score.\n",
    "    \"\"\"\n",
    "    # Sort logits in descending order and get their original indices\n",
    "    sorted_indices = torch.argsort(logits, descending=True)\n",
    "    \n",
    "    # Find the rank of the target token(s)\n",
    "    ranks = []\n",
    "    for t in token_ids:\n",
    "        \n",
    "        if t in sorted_indices:\n",
    "            rank = (sorted_indices == t).nonzero().item() + 1\n",
    "            ranks.append(rank)\n",
    "        else:\n",
    "            \n",
    "            print(f\"Token ID {t} not found in logits. Returning a high rank.\", file=sys.stderr)\n",
    "            return len(logits) + 1 \n",
    "    \n",
    "    if not ranks:\n",
    "        return len(logits) + 1\n",
    "        \n",
    "    return min(ranks)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78cac20b-3613-4eb6-afc3-9f616057c00f",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ef508-30f7-44b1-b889-163ff3793af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_prompt_attention_only(model, tokenizer, prompt_df):\n",
    "    \"\"\"\n",
    "    Analyzes a DataFrame of prompts by calculating logit scores and ranks for each layer,\n",
    "    based on the output of the attention mechanism only.\n",
    "    \n",
    "    Args:\n",
    "        model: The Llama 2 model loaded with Transformer Lens.\n",
    "        tokenizer: The tokenizer.\n",
    "        prompt_df : The DataFrame containing 'constrained prompt',\n",
    "                                  'emotion', and 'predicted emotion' columns.\n",
    "                                  \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the average logit scores and ranks per layer.\n",
    "    \"\"\"\n",
    "    all_metrics = []\n",
    "    \n",
    "    for _, row in prompt_df.iterrows():\n",
    "        try:\n",
    "            true_ids = get_token_ids(tokenizer, row['emotion'])\n",
    "            predicted_ids = get_token_ids(tokenizer, row['predicted emotion'])\n",
    "            \n",
    "            input_ids = tokenizer.encode(row['constrained prompt'], return_tensors='pt')\n",
    "            final_token_idx = input_ids.shape[-1] - 1\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits, cache = model.run_with_cache(row['constrained prompt'])\n",
    "\n",
    "            for layer_idx in range(model.cfg.n_layers):\n",
    "                \n",
    "                attn_out_contribution = cache[(\"attn_out\", layer_idx)][0, final_token_idx, :]\n",
    "\n",
    "            \n",
    "                attn_logits = model.unembed(attn_out_contribution)\n",
    "                \n",
    "                # Sum of the logits for the true and predicted emotion tokens.\n",
    "                true_logit_raw = attn_logits[true_ids].sum().item()\n",
    "                predicted_logit_raw = attn_logits[predicted_ids].sum().item()\n",
    "                logit_difference = predicted_logit_raw - true_logit_raw\n",
    "                \n",
    "                # rank for the true and predicted tokens\n",
    "                true_rank = get_rank(attn_logits, true_ids)\n",
    "                predicted_rank = get_rank(attn_logits, predicted_ids)\n",
    "                \n",
    "                all_metrics.append({\n",
    "                    'layer': layer_idx,\n",
    "                    'true_logit_raw': true_logit_raw,\n",
    "                    'predicted_logit_raw': predicted_logit_raw,\n",
    "                    'logit_difference': logit_difference\n",
    "                    #'true_rank': true_rank,\n",
    "                    #'predicted_rank': predicted_rank\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping analysis for prompt: {row['constrained prompt']}\\nError: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_metrics:\n",
    "        print(\"No metrics were generated. The prompt DataFrame might be empty or a tokenization error occurred.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    all_metrics_df = pd.DataFrame(all_metrics)\n",
    "    average_metrics_df = all_metrics_df.groupby('layer').mean().reset_index()\n",
    "    \n",
    "    return average_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd97a9c-6049-477e-a528-c666d809b537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_prompt_set_mlp_only(model, tokenizer, prompt_df):\n",
    "    \"\"\"\n",
    "    Analyzes a DataFrame of prompts by calculating logit scores and ranks for each layer.\n",
    "    \n",
    "    Args:\n",
    "        model (HookedTransformer): The Llama 2 model loaded with Transformer Lens.\n",
    "        tokenizer: The tokenizer.\n",
    "        prompt_df (pd.DataFrame): The DataFrame containing 'constrained prompt',\n",
    "                                    'emotion', and 'predicted emotion' columns.\n",
    "                                    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the average logit scores and ranks per layer.\n",
    "    \"\"\"\n",
    "    all_metrics = []\n",
    "    \n",
    "    for _, row in prompt_df.iterrows():\n",
    "        try:\n",
    "            true_ids = get_token_ids(tokenizer, row['emotion'])\n",
    "            predicted_ids = get_token_ids(tokenizer, row['predicted emotion'])\n",
    "            \n",
    "            input_ids = tokenizer.encode(row['constrained prompt'], return_tensors='pt')\n",
    "            final_token_idx = input_ids.shape[-1] - 1\n",
    "            \n",
    "           \n",
    "            with torch.no_grad():\n",
    "                logits, cache = model.run_with_cache(row['constrained prompt'])\n",
    "\n",
    "            for layer_idx in range(model.cfg.n_layers):\n",
    "                \n",
    "                #  output of the MLP for the last token\n",
    "                mlp_out_contribution = cache[(\"mlp_out\", layer_idx)][0, final_token_idx, :]\n",
    "\n",
    "                # MLP contribution through the unembedding matrix to get logit contribution\n",
    "               \n",
    "                mlp_logits = model.unembed(mlp_out_contribution)\n",
    "                \n",
    "                # Sum of the logits for the true and predicted emotion tokens.\n",
    "                true_logit_raw = mlp_logits[true_ids].sum().item()\n",
    "                predicted_logit_raw = mlp_logits[predicted_ids].sum().item()\n",
    "                logit_difference = predicted_logit_raw - true_logit_raw\n",
    "                \n",
    "                #rank for the true and predicted tokens\n",
    "                true_rank = get_rank(mlp_logits, true_ids)\n",
    "                predicted_rank = get_rank(mlp_logits, predicted_ids)\n",
    "                \n",
    "                all_metrics.append({\n",
    "                    'layer': layer_idx,\n",
    "                    'true_logit_raw': true_logit_raw,\n",
    "                    'predicted_logit_raw': predicted_logit_raw,\n",
    "                    'logit_difference': logit_difference\n",
    "                    #'true_rank': true_rank,\n",
    "                    #'predicted_rank': predicted_rank\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping analysis for prompt: {row['constrained prompt']}\\nError: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_metrics:\n",
    "        print(\"No metrics were generated. The prompt DataFrame might be empty or a tokenization error occurred.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    all_metrics_df = pd.DataFrame(all_metrics)\n",
    "    average_metrics_df = all_metrics_df.groupby('layer').mean().reset_index()\n",
    "    \n",
    "    return average_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032bbce-48aa-43c6-8d1c-0aaa59de770d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_avg_logit_metrics_set1 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set1_df)\n",
    "\n",
    "attn_avg_logit_metrics_set2 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set2_df)\n",
    "\n",
    "attn_avg_logit_metrics_set3 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set3_df)\n",
    "\n",
    "attn_avg_logit_metrics_set4 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set4_df)\n",
    "\n",
    "attn_avg_logit_metrics_set5 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set5_df)\n",
    "\n",
    "attn_avg_logit_metrics_set6 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set6_df)\n",
    "\n",
    "attn_avg_logit_metrics_set7 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set7_df)\n",
    "\n",
    "attn_avg_logit_metrics_set8 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set8_df)\n",
    "\n",
    "attn_avg_logit_metrics_set9 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set9_df)\n",
    "\n",
    "attn_avg_logit_metrics_set10 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set10_df)\n",
    "\n",
    "attn_avg_logit_metrics_set11 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set11_df)\n",
    "\n",
    "attn_avg_logit_metrics_set12 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set12_df)\n",
    "\n",
    "attn_avg_logit_metrics_set13 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set13_df)\n",
    "\n",
    "attn_avg_logit_metrics_set14 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set14_df)\n",
    "\n",
    "attn_avg_logit_metrics_set15 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set15_df)\n",
    "\n",
    "attn_avg_logit_metrics_set16 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set16_df)\n",
    "\n",
    "attn_avg_logit_metrics_set17 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set17_df)\n",
    "\n",
    "attn_avg_logit_metrics_set18 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set18_df)\n",
    "\n",
    "attn_avg_logit_metrics_set19 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set19_df)\n",
    "\n",
    "attn_avg_logit_metrics_set20 = analyze_prompt_attention_only(model_llama, model_llama.tokenizer, hallu_set20_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e3967-0047-4db7-92cb-4ed3d2dcd286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_avg_logit_metrics_set8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0ac62-bc37-444a-9bc8-673ebce48a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_prompt_set_single_mlp_only(model, tokenizer, prompt, true_emotion, predicted_emotion):\n",
    "    \"\"\"\n",
    "    Analyzes a single prompt by calculating MLP logit scores and ranks for each layer.\n",
    "\n",
    "    Args:\n",
    "        model : The Llama 2 model loaded with Transformer Lens.\n",
    "        tokenizer: The tokenizer.\n",
    "        prompt (str): The constrained prompt text.\n",
    "        true_emotion (str): The correct emotion label.\n",
    "        predicted_emotion (str): The model's hallucinated emotion label.\n",
    "                                        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the logit scores and ranks per layer.\n",
    "    \"\"\"\n",
    "    all_metrics = []\n",
    "    \n",
    "    try:\n",
    "        true_ids = get_token_ids(tokenizer, true_emotion)\n",
    "        predicted_ids = get_token_ids(tokenizer, predicted_emotion)\n",
    "        \n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        final_token_idx = input_ids.shape[-1] - 1\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, cache = model.run_with_cache(prompt)\n",
    "\n",
    "        for layer_idx in range(model.cfg.n_layers):\n",
    "            # output of the MLP for the last token\n",
    "            mlp_out_contribution = cache[(\"mlp_out\", layer_idx)][0, final_token_idx, :]\n",
    "\n",
    "            # MLP contribution through the unembedding matrix\n",
    "            mlp_logits = model.unembed(mlp_out_contribution)\n",
    "            \n",
    "            # Sum of the logits for the true and predicted emotion tokens.\n",
    "            true_logit_raw = mlp_logits[true_ids].sum().item()\n",
    "            predicted_logit_raw = mlp_logits[predicted_ids].sum().item()\n",
    "            logit_difference = predicted_logit_raw - true_logit_raw\n",
    "            \n",
    "            #rank for the true and predicted tokens\n",
    "            true_rank = get_rank(mlp_logits, true_ids)\n",
    "            predicted_rank = get_rank(mlp_logits, predicted_ids)\n",
    "            \n",
    "            all_metrics.append({\n",
    "                'layer': layer_idx,\n",
    "                'true_logit_raw': true_logit_raw,\n",
    "                'predicted_logit_raw': predicted_logit_raw,\n",
    "                'logit_difference': logit_difference\n",
    "                #'true_rank': true_rank,\n",
    "                #'predicted_rank': predicted_rank\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to analyze prompt: {prompt}\\nError: {e}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15932220-1e01-4635-953a-92429b19a222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_attention_distinction(model, tokenizer, prompt_df, distractor_count=100):\n",
    "    \"\"\"\n",
    "    Analyzes a DataFrame of prompts by computing the relative attention-extracted\n",
    "    attribute information, I_a^(l)(o), for each layer as described by the paper.\n",
    "\n",
    "    Args:\n",
    "        model : The loaded model.\n",
    "        tokenizer: The tokenizer.\n",
    "        prompt_df : The DataFrame with prompts and emotions.\n",
    "        distractor_count (int): The number of top tokens to use as distractors.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the average distinction scores per layer.\n",
    "    \"\"\"\n",
    "    all_metrics = []\n",
    "    unembedding_matrix = model.unembed.W_U.squeeze() # Get the unembedding matrix\n",
    "    \n",
    "    for _, row in prompt_df.iterrows():\n",
    "        try:\n",
    "            true_emotion_text = row['emotion']\n",
    "            prompt_text = row['constrained prompt']\n",
    "\n",
    "            true_ids = get_token_ids(tokenizer, true_emotion_text)\n",
    "            \n",
    "            input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "            final_token_idx = input_ids.shape[-1] - 1\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits, cache = model.run_with_cache(prompt_text)\n",
    "\n",
    "            for layer_idx in range(model.cfg.n_layers):\n",
    "                #Use the MLP output to find top distractors\n",
    "                \n",
    "                mlp_out_contribution = cache[(\"mlp_out\", layer_idx)][0, final_token_idx, :]\n",
    "                mlp_logits = model.unembed(mlp_out_contribution)\n",
    "                \n",
    "                #top N tokens that have high MLP-enriched information\n",
    "                _, top_distractor_ids = torch.topk(mlp_logits, k=distractor_count)\n",
    "                \n",
    "                # the unembedding vectors for the true token and the distractors\n",
    "                true_unembedding_vector = unembedding_matrix[true_ids].mean(dim=0)\n",
    "                \n",
    "                # the unembedding vectors for all top distractors\n",
    "                distractor_unembedding_vectors = unembedding_matrix[top_distractor_ids]\n",
    "                mean_distractor_vector = distractor_unembedding_vectors.mean(dim=0)\n",
    "                \n",
    "                # the difference vector (e_o - e_bar_o')\n",
    "                distinction_vector = true_unembedding_vector - mean_distractor_vector\n",
    "                \n",
    "                #the attention output vector (a_T)\n",
    "                attn_out_contribution = cache[(\"attn_out\", layer_idx)][0, final_token_idx, :]\n",
    "                \n",
    "                # (a_T * (e_o - e_bar_o'))\n",
    "                distinction_score = torch.dot(attn_out_contribution, distinction_vector).item()\n",
    "                \n",
    "                all_metrics.append({\n",
    "                    'layer': layer_idx,\n",
    "                    'distinction_score': distinction_score,\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping analysis for prompt: {row['constrained prompt']}\\nError: {e}\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "    if not all_metrics:\n",
    "        print(\"No metrics were generated. The DataFrame might be empty or an error occurred.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    all_metrics_df = pd.DataFrame(all_metrics)\n",
    "    average_metrics_df = all_metrics_df.groupby('layer').mean().reset_index()\n",
    "    \n",
    "    return average_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f5479-da0f-42e8-b82c-b178e98fd951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_final_embed_prompt_set(model, tokenizer, prompt_df):\n",
    "    \"\"\"\n",
    "    Analyzes a DataFrame of prompts by calculating logit scores and ranks for each layer.\n",
    "    \n",
    "    Args:\n",
    "        model : The Llama 2 model loaded with Transformer Lens.\n",
    "        tokenizer: The tokenizer.\n",
    "        prompt_df: The DataFrame containing 'constrained prompt',\n",
    "                                    'emotion', and 'predicted emotion' columns.\n",
    "                                    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the average logit scores and ranks per layer.\n",
    "    \"\"\"\n",
    "    all_metrics = []\n",
    "    \n",
    "    for _, row in prompt_df.iterrows():\n",
    "        try:\n",
    "            true_ids = get_token_ids(tokenizer, row['emotion'])\n",
    "            predicted_ids = get_token_ids(tokenizer, row['prompt response'])\n",
    "            \n",
    "            input_ids = tokenizer.encode(row['constrained prompt'], return_tensors='pt')\n",
    "            final_token_idx = input_ids.shape[-1] - 1\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits, cache = model.run_with_cache(row['constrained prompt'])\n",
    "\n",
    "            for layer_idx in range(model.cfg.n_layers):\n",
    "                #logit lens:\n",
    "                current_residual_stream = cache[(\"resid_post\", layer_idx)]\n",
    "                layer_logits = model.ln_final(current_residual_stream)\n",
    "                layer_logits = model.unembed(layer_logits)\n",
    "                layer_logits_final_token = layer_logits[0, final_token_idx, :]\n",
    "                \n",
    "                # Sum of the logits for the true and predicted emotion tokens.\n",
    "                true_logit_raw = layer_logits_final_token[true_ids].sum().item()\n",
    "                predicted_logit_raw = layer_logits_final_token[predicted_ids].sum().item()\n",
    "                logit_difference = predicted_logit_raw - true_logit_raw\n",
    "                \n",
    "                #the rank for the true and predicted tokens\n",
    "                true_rank = get_rank(layer_logits_final_token, true_ids)\n",
    "                predicted_rank = get_rank(layer_logits_final_token, predicted_ids)\n",
    "                \n",
    "                all_metrics.append({\n",
    "                    'layer': layer_idx,\n",
    "                    'true_logit_raw': true_logit_raw,\n",
    "                    'predicted_logit_raw': predicted_logit_raw,\n",
    "                    'logit_difference': logit_difference\n",
    "                  \n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping analysis for prompt: {row['constrained prompt']}\\nError: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_metrics:\n",
    "        print(\"No metrics were generated. The prompt DataFrame might be empty or a tokenization error occurred.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    all_metrics_df = pd.DataFrame(all_metrics)\n",
    "    average_metrics_df = all_metrics_df.groupby('layer').mean().reset_index()\n",
    "    \n",
    "    return all_metrics_df, average_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d3ad7-ce77-44b7-9d04-373f8d1b9848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_logit_metrics_set1 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set1_df)\n",
    "\n",
    "avg_logit_metrics_set2 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set2_df)\n",
    "\n",
    "avg_logit_metrics_set3 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set3_df)\n",
    "\n",
    "avg_logit_metrics_set4 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set4_df)\n",
    "\n",
    "avg_logit_metrics_set5 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set5_df)\n",
    "\n",
    "avg_logit_metrics_set6 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set6_df)\n",
    "\n",
    "avg_logit_metrics_set7 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set7_df)\n",
    "\n",
    "avg_logit_metrics_set8 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set8_df)\n",
    "\n",
    "avg_logit_metrics_set9 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set9_df)\n",
    "\n",
    "avg_logit_metrics_set10 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set10_df)\n",
    "\n",
    "\n",
    "avg_logit_metrics_set11 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set11_df)\n",
    "\n",
    "avg_logit_metrics_set12 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set12_df)\n",
    "\n",
    "avg_logit_metrics_set13 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set13_df)\n",
    "\n",
    "avg_logit_metrics_set14 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set14_df)\n",
    "\n",
    "avg_logit_metrics_set15 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set15_df)\n",
    "\n",
    "avg_logit_metrics_set16 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set16_df)\n",
    "\n",
    "avg_logit_metrics_set17 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set17_df)\n",
    "\n",
    "avg_logit_metrics_set18 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set18_df)\n",
    "\n",
    "avg_logit_metrics_set19 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set19_df)\n",
    "\n",
    "avg_logit_metrics_set20 = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set20_df)\n",
    "\n",
    "\n",
    "avg_logit_metrics_set_unknown = analyze_prompt_set_mlp_only(model_llama, model_llama.tokenizer, hallu_set20_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee55bbc-56e0-443d-9ee1-7fb33f8fb3e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_logit_metrics_set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64d893-c1ac-4801-95b6-e9a043be81d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_avg_mlp_dfs = [\n",
    "    avg_logit_metrics_set1, avg_logit_metrics_set2, avg_logit_metrics_set3,\n",
    "    avg_logit_metrics_set4, avg_logit_metrics_set5, avg_logit_metrics_set6,\n",
    "    avg_logit_metrics_set7, avg_logit_metrics_set8, avg_logit_metrics_set9,\n",
    "    avg_logit_metrics_set10, avg_logit_metrics_set11, avg_logit_metrics_set12,\n",
    "    avg_logit_metrics_set13, avg_logit_metrics_set14, avg_logit_metrics_set15,\n",
    "    avg_logit_metrics_set16, avg_logit_metrics_set17, avg_logit_metrics_set18,\n",
    "    avg_logit_metrics_set19, avg_logit_metrics_set20\n",
    "]\n",
    "\n",
    "hallucination_labels = [\n",
    "    'joy-sadness', 'joy-love', 'joy-fear', 'joy-surprise', 'joy-anger',\n",
    "    'anger-sadness', 'anger-joy', 'anger-fear', 'anger-love', 'anger-surprise',\n",
    "    'sadness-fear', 'sadness-love', 'sadness-surprise', 'sadness-joy', 'sadness-anger',\n",
    "    'fear-sadness', 'fear-love', 'fear-anger', 'fear-surprise', 'fear-joy',\n",
    "    'joy-unknown'\n",
    "]\n",
    "\n",
    "\n",
    "combined_avg_mlp_df = pd.DataFrame()\n",
    "for df, label in zip(all_avg_mlp_dfs, hallucination_labels):\n",
    "    df['hallucination_type'] = label\n",
    "    combined_avg_mlp_df = pd.concat([combined_avg_mlp_df, df])\n",
    "\n",
    "\n",
    "heatmap_mlp_data = combined_avg_mlp_df.pivot(index='hallucination_type', columns='layer', values='logit_difference')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.heatmap(heatmap_mlp_data, cmap='coolwarm', center=0, annot=False, fmt=\".2f\",\n",
    "            linewidths=0.5, linecolor='gray', cbar_kws={'label': 'Average Logit Difference (y ratio)'})\n",
    "\n",
    "plt.title('Average Hallucination Trajectories MLP', fontsize=16)\n",
    "plt.xlabel('Layer Number', fontsize=14)\n",
    "plt.ylabel('Hallucination Type', fontsize=14)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3cb19-b50b-4618-a28c-3843d67aa137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_metrics_by_layer = pd.DataFrame(metrics_by_layer)\n",
    "df_metrics_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1052102-c4f9-4498-ac6d-61116786d00c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final_embed_metrics1, df_avg_final_embed_metrics1 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set1_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics2 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set2_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics3 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set3_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics4 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set4_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics5 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set5_df)\n",
    "\n",
    "\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics6 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set6_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics7 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set7_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics8 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set8_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics9 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set9_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics10 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set10_df)\n",
    "\n",
    "\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics11 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set11_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics12 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set12_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics13 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set13_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics14 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set14_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics15 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set15_df)\n",
    "\n",
    "\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics16 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set16_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics17 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set17_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics18 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set18_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics19 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set19_df)\n",
    "\n",
    "df_final_embed_metrics1, df_avg_final_embed_metrics20 = analyze_final_embed_prompt_set(model_llama, tokenizer, hallu_set20_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984cb3f-0109-4aee-a2cc-248d12e2d0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_avg_final_embed_metrics20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ba9a2-27f9-4888-9248-3ee43e3808be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_avg_dfs = [\n",
    "    df_avg_final_embed_metrics1, df_avg_final_embed_metrics2, df_avg_final_embed_metrics3,\n",
    "    df_avg_final_embed_metrics4, df_avg_final_embed_metrics5, df_avg_final_embed_metrics6,\n",
    "    df_avg_final_embed_metrics7, df_avg_final_embed_metrics8, df_avg_final_embed_metrics9,\n",
    "    df_avg_final_embed_metrics10, df_avg_final_embed_metrics11, df_avg_final_embed_metrics12,\n",
    "    df_avg_final_embed_metrics13, df_avg_final_embed_metrics14, df_avg_final_embed_metrics15,\n",
    "    df_avg_final_embed_metrics16, df_avg_final_embed_metrics17, df_avg_final_embed_metrics18,\n",
    "    df_avg_final_embed_metrics19, df_avg_final_embed_metrics20\n",
    "]\n",
    "\n",
    "hallucination_labels = [\n",
    "    'joy-sadness', 'joy-love', 'joy-fear', 'joy-surprise', 'joy-anger',\n",
    "    'anger-sadness', 'anger-joy', 'anger-fear', 'anger-love', 'anger-surprise',\n",
    "    'sadness-fear', 'sadness-love', 'sadness-surprise', 'sadness-joy', 'sadness-anger',\n",
    "    'fear-sadness', 'fear-love', 'fear-anger', 'fear-surprise', 'fear-joy',\n",
    "    'joy-unknown'\n",
    "]\n",
    "\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "for df, label in zip(all_avg_dfs, hallucination_labels):\n",
    "    df['hallucination_type'] = label\n",
    "    combined_df = pd.concat([combined_df, df])\n",
    "\n",
    "\n",
    "heatmap_data = combined_df.pivot(index='hallucination_type', columns='layer', values='logit_difference')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.heatmap(heatmap_data, cmap='coolwarm', center=0, annot=False, fmt=\".2f\",\n",
    "            linewidths=0.5, linecolor='gray', cbar_kws={'label': 'Average Logit Difference (y ratio)'})\n",
    "\n",
    "plt.title('Average Hallucination Trajectories', fontsize=16)\n",
    "plt.xlabel('Layer Number', fontsize=14)\n",
    "plt.ylabel('Hallucination Type', fontsize=14)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dab124-aefb-4fac-991f-2d839f605bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Plot: Raw Logit Scores vs. Layer ---\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "\n",
    "plt.plot(avg_logit_metrics_set1['layer'], avg_logit_metrics_set1['true_logit_raw'], label='True Emotion Logit', marker='o', linestyle='-', color='blue')\n",
    "plt.plot(avg_logit_metrics_set1['layer'],avg_logit_metrics_set1['predicted_logit_raw'], label='Predicted Emotion Logit', marker='x', linestyle='--', color='orange')\n",
    "\n",
    "\n",
    "plt.title('Average Raw Logit Scores Across All Layers Set 1 Joy->Sadness', fontsize=16)\n",
    "plt.xlabel('Layers', fontsize=12)\n",
    "plt.ylabel('Logit Score', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, which='both', linestyle=':', linewidth=0.5)\n",
    "\n",
    "\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "\n",
    "plt.plot(avg_logit_metrics_set1['layer'], avg_logit_metrics_set1['logit_difference'], label='Logit Difference (Predicted - True)', marker='s', linestyle='-', color='red')\n",
    "\n",
    "\n",
    "plt.title('Average Logit Difference Across Layers', fontsize=16)\n",
    "plt.xlabel('Layer', fontsize=12)\n",
    "plt.ylabel('Logit Difference', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, which='both', linestyle=':', linewidth=0.5)\n",
    "\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdafc1-b507-4ec6-a097-cc499d435267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Plot: Raw Logit Scores vs. Layer ---\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "\n",
    "plt.plot(avg_logit_metrics_set2['layer'], avg_logit_metrics_set2['true_logit_raw'], label='True Emotion Logit', marker='o', linestyle='-', color='blue')\n",
    "plt.plot(avg_logit_metrics_set2['layer'],avg_logit_metrics_set2['predicted_logit_raw'], label='Predicted Emotion Logit', marker='x', linestyle='--', color='orange')\n",
    "\n",
    "\n",
    "plt.title('Average Raw Logit Scores Across All Layers Set 2', fontsize=16)\n",
    "plt.xlabel('Layers', fontsize=12)\n",
    "plt.ylabel('Logit Score', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, which='both', linestyle=':', linewidth=0.5)\n",
    "\n",
    "\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "\n",
    "plt.plot(avg_logit_metrics_set2['layer'], avg_logit_metrics_set2['logit_difference'], label='Logit Difference (Predicted - True)', marker='s', linestyle='-', color='red')\n",
    "\n",
    "\n",
    "plt.title('Average Logit Difference Across Layers', fontsize=16)\n",
    "plt.xlabel('Layer', fontsize=12)\n",
    "plt.ylabel('Logit Difference', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, which='both', linestyle=':', linewidth=0.5)\n",
    "\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8101640-fe45-4b3e-9308-380f4cd7adea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "\n",
    "plt.plot(avg_logit_metrics_set1['layer'], avg_logit_metrics_set3['true_logit_raw'], label='True Emotion Logit', marker='o', linestyle='-', color='blue')\n",
    "plt.plot(avg_logit_metrics_set1['layer'],avg_logit_metrics_set3['predicted_logit_raw'], label='Predicted Emotion Logit', marker='x', linestyle='--', color='orange')\n",
    "\n",
    "\n",
    "plt.title('Average Raw Logit Scores Across All Layers Set 1 Joy->Fear', fontsize=16)\n",
    "plt.xlabel('Layers', fontsize=12)\n",
    "plt.ylabel('Logit Score', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, which='both', linestyle=':', linewidth=0.5)\n",
    "\n",
    "\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "\n",
    "plt.plot(avg_logit_metrics_set1['layer'], avg_logit_metrics_set1['logit_difference'], label='Logit Difference (Predicted - True)', marker='s', linestyle='-', color='red')\n",
    "\n",
    "\n",
    "plt.title('Average Logit Difference Across Layers', fontsize=16)\n",
    "plt.xlabel('Layer', fontsize=12)\n",
    "plt.ylabel('Logit Difference', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, which='both', linestyle=':', linewidth=0.5)\n",
    "\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102eb8ae-f8ec-4c87-ade2-be671d2a2a32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_prompt_ranks(model, tokenizer, prompt_df):\n",
    "    \"\"\"\n",
    "    Analyzes a DataFrame of prompts by calculating the minimum rank for\n",
    "    each prompt across all layers.\n",
    "    \"\"\"\n",
    "    categorized_prompts = []\n",
    "    \n",
    "    for index, row in prompt_df.iterrows():\n",
    "        try:\n",
    "            true_emotion_text = row['emotion']\n",
    "            prompt_text = row['constrained prompt']\n",
    "\n",
    "            # token IDs for the true emotion\n",
    "            true_ids = get_token_ids(tokenizer, true_emotion_text)\n",
    "            \n",
    "            input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "            final_token_idx = input_ids.shape[-1] - 1\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits, cache = model.run_with_cache(prompt_text)\n",
    "\n",
    "            min_true_rank_across_layers = float('inf')\n",
    "            min_rank_layer_idx = -1\n",
    "            \n",
    "            for layer_idx in range(model.cfg.n_layers):\n",
    "                # output of the MLP for the last token\n",
    "                mlp_out_contribution = cache[(\"mlp_out\", layer_idx)][0, final_token_idx, :]\n",
    "\n",
    "                # MLP contribution through the unembedding matrix\n",
    "                mlp_logits = model.unembed(mlp_out_contribution)\n",
    "                \n",
    "                # the rank for the true token\n",
    "                true_rank = get_rank(mlp_logits, true_ids)\n",
    "                \n",
    "                # Update the minimum rank and the layer index\n",
    "                if true_rank < min_true_rank_across_layers:\n",
    "                    min_true_rank_across_layers = true_rank\n",
    "                    min_rank_layer_idx = layer_idx\n",
    "                \n",
    "          \n",
    "                \n",
    "            categorized_prompts.append({\n",
    "                'prompt': prompt_text,\n",
    "                'rank last layer': true_rank,\n",
    "                'min_true_rank': min_true_rank_across_layers,\n",
    "                'min_rank_layer': min_rank_layer_idx\n",
    "                \n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping analysis for row {index}. Error: {e}\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "    if not categorized_prompts:\n",
    "        print(\"No prompts were categorized. The DataFrame might be empty or an error occurred.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    categorized_df = pd.DataFrame(categorized_prompts)\n",
    "        \n",
    "    return categorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c224cd-7846-47e0-8f5f-d6bfc04350d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prompt_ranks_df = calculate_prompt_ranks(model_llama, tokenizer, hallu_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8b163f-ef7a-4e40-8f60-a4d60ec91dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_ranks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0958afc-814a-4b18-aa26-dea3d20c1aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def categorize_prompts(ranked_df, threshold_strategy=\"average\"):\n",
    "    \"\"\"\n",
    "    Calculates the median of 'min_true_rank' as a threshold and\n",
    "    categorizes the type of hallucination for each prompt.\n",
    "\n",
    "    Args:\n",
    "        ranked_df (pd.DataFrame): DataFrame containing prompt analysis with\n",
    "                                   'min_true_rank' and other rank columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with two new columns:\n",
    "                      'threshold' and 'hallucination type'.\n",
    "    \"\"\"\n",
    "    if ranked_df.empty:\n",
    "        print(\"Input DataFrame is empty. Cannot categorize prompts.\", file=sys.stderr)\n",
    "        return ranked_df\n",
    "\n",
    "    # Calculate the median of the 'min_true_rank' column to use as the threshold\n",
    "    try:\n",
    "        knowledge_threshold = np.median(ranked_df['min_true_rank'])\n",
    "    except KeyError as e:\n",
    "        print(f\"DataFrame is missing the required column: {e}\", file=sys.stderr)\n",
    "        return ranked_df\n",
    "\n",
    "    # Add the threshold as a new column for visibility\n",
    "    ranked_df['threshold'] = knowledge_threshold\n",
    "\n",
    "    # Define a function to assign the hallucination type\n",
    "    def assign_category(min_rank):\n",
    "        if min_rank <= knowledge_threshold:\n",
    "            return \"Extraction\"\n",
    "        else:\n",
    "            return \"Enrichment\"\n",
    "\n",
    "    # Apply the categorization function to create the new column\n",
    "    ranked_df['hallucination type'] = ranked_df['min_true_rank'].apply(assign_category)\n",
    "    \n",
    "    if 'hallucination_type' in ranked_df.columns:\n",
    "        ranked_df = ranked_df.drop(columns=['hallucination_type'])\n",
    "\n",
    "    return ranked_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e013eec-4257-4ab6-9180-fa9957b30bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorized_hallu = categorize_prompts(prompt_ranks_df, threshold_strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ea8a5-dbb7-481d-b584-d5c9620e7ded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorized_hallu#[categorized_hallu['hallucination type']=='Extraction'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fbbf08-614c-4ab5-b9c6-89b57ffb9fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "knowledge_threshold = 1\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(categorized_hallu['min_rank_layer'], categorized_hallu['min_true_rank'], alpha=0.8)\n",
    "\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "\n",
    "plt.axhline(y=knowledge_threshold, color='r', linestyle='--', label=f'Knowledge Threshold ({knowledge_threshold})')\n",
    "\n",
    "\n",
    "plt.title('Minimum True Rank per Layer for Hallucinating Prompts', fontsize=16)\n",
    "plt.xlabel('Layer Index (min rank)', fontsize=12)\n",
    "plt.ylabel('Minimum True Rank (log scale)', fontsize=12)\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(0, categorized_hallu['min_rank_layer'].max() + 1, 1))\n",
    "\n",
    "\n",
    "plt.grid(True, which=\"both\", ls=\"--\", c='0.7')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c653cca-03f3-4459-860f-e54d0066f048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Plot : Distribution of Layers ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(data=categorized_hallu, x='min_rank_layer', kde=False, bins=20)\n",
    "plt.title('Distribution of Layers with Minimum Rank')\n",
    "plt.xlabel('Layer Index')\n",
    "plt.ylabel('Num of Prompts')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907bd00b-3b15-46ab-8507-2774ab0ca2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.kdeplot(data=categorized_hallu, x='min_true_rank', fill=True)\n",
    "plt.title('Distribution of Minimum True Ranks')\n",
    "plt.xlabel('Minimum True Rank')\n",
    "plt.ylabel('Density')\n",
    "plt.xscale('log') \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10ed32-ab93-480f-9eb3-855084cc63ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_and_categorize_prompts_attention_only(model, tokenizer, prompt_df, extraction_threshold=320):\n",
    "    \"\"\"\n",
    "    Analyzes a DataFrame of prompts by calculating the minimum rank for\n",
    "    the true emotion using an attention-only logit lens, then categorizes\n",
    "    the hallucination type.\n",
    "\n",
    "    Args:\n",
    "        model : The loaded model.\n",
    "        tokenizer: The tokenizer.\n",
    "        prompt_df : The DataFrame with prompts and emotions.\n",
    "        extraction_threshold (int): The rank threshold for a successful\n",
    "                                    attention-based extraction.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with each prompt's calculated ranks\n",
    "                      and categorized hallucination type.\n",
    "    \"\"\"\n",
    "    categorized_prompts = []\n",
    "    \n",
    "    for index, row in prompt_df.iterrows():\n",
    "        try:\n",
    "            true_emotion_text = row['emotion']\n",
    "            prompt_text = row['constrained prompt']\n",
    "\n",
    "            #token IDs for the true emotion\n",
    "            true_ids = get_token_ids(tokenizer, true_emotion_text)\n",
    "            \n",
    "            input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "            final_token_idx = input_ids.shape[-1] - 1\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits, cache = model.run_with_cache(prompt_text)\n",
    "\n",
    "            min_true_rank_across_layers = float('inf')\n",
    "            min_rank_layer_idx = -1\n",
    "            \n",
    "            for layer_idx in range(model.cfg.n_layers):\n",
    "               \n",
    "                attn_out_contribution = cache[(\"attn_out\", layer_idx)][0, final_token_idx, :]\n",
    "\n",
    "                # attention contribution through the unembedding matrix\n",
    "                attn_logits = model.unembed(attn_out_contribution)\n",
    "                \n",
    "                # rank for the true token based on the attention logits\n",
    "                true_rank = get_rank(attn_logits, true_ids)\n",
    "                \n",
    "                # Update the minimum rank and the layer index\n",
    "                if true_rank < min_true_rank_across_layers:\n",
    "                    min_true_rank_across_layers = true_rank\n",
    "                    min_rank_layer_idx = layer_idx\n",
    "                \n",
    "            \n",
    "            if min_true_rank_across_layers < extraction_threshold:\n",
    "                \n",
    "                hallucination_type = \"Enrichment\"\n",
    "            else:\n",
    "               \n",
    "                hallucination_type = \"Extraction\"\n",
    "                \n",
    "            categorized_prompts.append({\n",
    "                'prompt': prompt_text,\n",
    "                'min_true_rank': min_true_rank_across_layers,\n",
    "                'min_rank_layer': min_rank_layer_idx,\n",
    "                'hallucination_type': hallucination_type\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping analysis for row {index}. Error: {e}\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "    if not categorized_prompts:\n",
    "        print(\"No prompts were categorized. The DataFrame might be empty or an error occurred.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    categorized_df = pd.DataFrame(categorized_prompts)\n",
    "    \n",
    "    return categorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa30e8-9ec0-4a20-b41a-2e593b1c671d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_categorized_df = analyze_and_categorize_prompts_attention_only(model_llama, model_llama.tokenizer, hallu_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b28f039-48c9-4616-863e-2270aedd1279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_categorized_df#['min_true_rank'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff15fff-bd79-46ab-a734-376231ddd2ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(model.cfg.n_layers), ranks_across_layers, marker='o', linestyle='-')\n",
    "plt.axhline(y=320, color='r', linestyle='--', label=f'Knowledge Threshold ({320)')\n",
    "            \n",
    "plt.title(f'Rank of Correct Emotion (\"{true_emotion_text}\") Across MLP Layers\\nPrompt {index}: {prompt_text[:50]}...')\n",
    "plt.xlabel('Layer Number')\n",
    "plt.ylabel('Rank (1-based)')\n",
    "plt.yscale('log') \n",
    "plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "plt.legend()           \n",
    "                              \n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7556e714-3709-4d8c-a865-4e6ccbbcb853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_logit_metrics_set2['predicted_rank'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79f383-2cc9-45e0-88a9-7f1102cd773c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e5c151-6dd9-4a32-9882-858c44dda091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "TARGET_SENTIMENTS = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'love', 'amusement', 'excitement',\n",
    "                     'happy', 'depressed', 'anxious', 'ecstatic', 'grief', 'calm', 'lonely', 'boredom',\n",
    "                     'elation', 'hope', 'disappointment', 'confusion', 'relief']\n",
    "\n",
    "\n",
    "COMMON_WORDS = [\n",
    "    \"the\", \"be\", \"to\", \"of\", \"and\", \"a\", \"in\", \"that\", \"have\", \"I\", \"it\", \"for\", \"not\", \"on\", \"with\", \"as\", \"do\",\n",
    "    \"at\", \"this\", \"but\", \"by\", \"from\", \"up\", \"so\", \"what\", \"we\", \"he\", \"she\", \"they\", \"was\", \"one\", \"all\", \"can\",\n",
    "    \"an\", \"is\", \"are\", \"you\", \"go\", \"new\", \"world\", \"time\", \"day\", \"night\", \"see\", \"man\", \"woman\", \"house\", \"car\",\n",
    "    \"computer\", \"phone\", \"food\", \"music\", \"art\", \"book\", \"story\", \"game\", \"city\", \"country\", \"people\", \"thing\",\n",
    "    \"life\", \"work\", \"school\", \"money\", \"power\", \"truth\", \"freedom\", \"justice\", \"peace\", \"war\", \"love\", \"hate\",\n",
    "    \"friend\", \"family\", \"child\", \"adult\", \"animal\", \"plant\", \"water\", \"fire\", \"earth\", \"sky\", \"sun\", \"moon\",\n",
    "    \"star\", \"space\", \"science\", \"math\", \"history\", \"language\", \"letter\", \"number\", \"idea\", \"thought\", \"feeling\",\n",
    "    \"emotion\", \"mind\", \"body\", \"health\", \"beauty\", \"ugly\", \"good\", \"bad\", \"right\", \"wrong\", \"old\", \"young\", \"big\",\n",
    "    \"small\", \"high\", \"low\", \"fast\", \"slow\", \"hot\", \"cold\", \"light\", \"dark\", \"open\", \"close\", \"start\", \"end\",\n",
    "    \"begin\", \"finish\", \"like\", \"dislike\", \"know\", \"think\", \"feel\", \"want\", \"need\", \"find\", \"give\", \"take\", \"come\",\n",
    "    \"go\", \"make\", \"do\", \"say\", \"tell\", \"ask\", \"answer\", \"look\", \"listen\", \"hear\", \"read\", \"write\", \"talk\", \"walk\",\n",
    "    \"run\", \"jump\", \"fly\", \"swim\", \"eat\", \"drink\", \"sleep\", \"dream\", \"wake\", \"laugh\", \"cry\", \"smile\", \"frown\",\n",
    "    \"win\", \"lose\", \"help\", \"thank\", \"sorry\", \"please\", \"maybe\", \"yes\", \"no\", \"why\", \"where\", \"when\", \"how\",\n",
    "    \"always\", \"never\", \"often\", \"sometimes\", \"seldom\", \"today\", \"tomorrow\", \"yesterday\"\n",
    "]\n",
    "\n",
    "\n",
    "FULL_VOCABULARY = sorted(list(set(TARGET_SENTIMENTS + COMMON_WORDS + [f\"token_{i}\" for i in range(10000)])))\n",
    "VOCAB_SIZE = len(FULL_VOCABULARY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54349e00-5c44-4d47-aeb5-5a99c5660688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simulated_scores = scores\n",
    "\n",
    "\n",
    "print(\"\\n--- Rho Star values for target sentiments ---\")\n",
    "for sentiment in TARGET_SENTIMENTS:\n",
    "    rho_val = rho_star(simulated_scores, sentiment)\n",
    "    print(f\"*('{sentiment}'): {rho_val:.4f}\")\n",
    "\n",
    "\n",
    "sorted_scores = sorted(simulated_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "\n",
    "print(\"\\n--- Top 10 Ranked Tokens in Simulated Vocabulary ---\")\n",
    "for rank, (token, score) in enumerate(sorted_scores[:10]):\n",
    "    print(f\"Rank {rank + 1}: '{token}' (Score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781b1ec4",
   "metadata": {},
   "source": [
    "## Attention Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2292bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c8d9e0-d8b6-452f-92c3-d22cd50c9e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_token_ids(tokenizer, emotion_string):\n",
    "    \"\"\"\n",
    "    Gets the token IDs for an emotion string by using the robust\n",
    "    tokenizer.encode() method. This function handles both single and multi-token\n",
    "    words and is less prone to failure than the tokenizer.tokenize() method.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer : The model's tokenizer.\n",
    "        emotion_string (str): The emotion word to tokenize.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of integers representing the token IDs for the emotion string.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If no tokens can be found for the emotion string.\n",
    "    \"\"\"\n",
    "    encoded_with_space = tokenizer.encode(f\" {emotion_string}\", add_special_tokens=False)\n",
    "    if encoded_with_space:\n",
    "        return encoded_with_space\n",
    "    \n",
    "    encoded_without_space = tokenizer.encode(emotion_string, add_special_tokens=False)\n",
    "    if encoded_without_space:\n",
    "        return encoded_without_space\n",
    "        \n",
    "    raise ValueError(f\"Could not find any tokens for '{emotion_string}'.\")\n",
    "\n",
    "def run_analysis_for_single_text(model, tokenizer, prompt, true_emotion, predicted_emotion):\n",
    "    \"\"\"\n",
    "    Runs a layer-by-layer analysis for a single text and returns the results.\n",
    "    This function now handles multi-token emotions by summing their logits.\n",
    "    \n",
    "    Args:\n",
    "        model : The loaded language model.\n",
    "        tokenizer : The model's tokenizer.\n",
    "        prompt (str): The full input prompt for the model.\n",
    "        true_emotion (str): The correct emotion label.\n",
    "        predicted_emotion (str): The emotion predicted by the model.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains the metrics\n",
    "              for a single layer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        true_ids = get_emotion_token_ids(tokenizer, true_emotion)\n",
    "        predicted_ids = get_emotion_token_ids(tokenizer, predicted_emotion)\n",
    "        \n",
    "        input_ids = tokenizer.to_tokens(prompt)\n",
    "        final_token_idx = input_ids.shape[-1] - 1\n",
    "        \n",
    "        logits, cache = model.run_with_cache(input_ids)\n",
    "\n",
    "        metrics_by_layer = []\n",
    "        \n",
    "        for layer_idx in range(model.cfg.n_layers + 1):\n",
    "            \n",
    "            current_residual_stream = cache[f'blocks.{layer_idx-1}.hook_resid_post'] if layer_idx > 0 else cache['hook_embed']\n",
    "            layer_logits = model.unembed(model.ln_final(current_residual_stream))\n",
    "            layer_logits_final_token = layer_logits[0, final_token_idx, :]\n",
    "            \n",
    "            true_logit = layer_logits_final_token[true_ids].sum().item()\n",
    "            predicted_logit = layer_logits_final_token[predicted_ids].sum().item()\n",
    "            \n",
    "            metrics_by_layer.append({\n",
    "                'layer': layer_idx,\n",
    "                'true_logit_raw': true_logit,\n",
    "                'predicted_logit_raw': predicted_logit\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Analysis failed for prompt: {prompt}\\nError: {e}\")\n",
    "        metrics_by_layer = []\n",
    "    \n",
    "    return metrics_by_layer\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-hf\",\n",
    "        fold_ln=False,\n",
    "        center_unembed=False,\n",
    "        center_writing_weights=False,\n",
    "    )\n",
    "    tokenizer = model.tokenizer\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    if torch.cuda.is_available():\n",
    "        model.to('cuda')\n",
    "    print(\"Model loaded for analysis.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")\n",
    " \n",
    "    model = None\n",
    "    tokenizer = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4eda28-065e-4970-898b-68a93fe80ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"\\nVisualizing Average Attention to Final Token across All Layers and Heads ---\")\n",
    "\n",
    "\n",
    "num_layers = model_llama.cfg.n_layers\n",
    "num_heads = model_llama.cfg.n_heads\n",
    "    \n",
    "    \n",
    "seq_len = cache[\"pattern\", 0].shape[-1]\n",
    "final_token_index = seq_len - 1\n",
    "\n",
    "  \n",
    "avg_attention_matrix = torch.zeros((num_heads, num_layers))\n",
    "\n",
    "\n",
    "for layer in range(num_layers):\n",
    "        #attention pattern shape is (batch, num_heads, seq_len, seq_len)\n",
    "    attention_patterns = cache[\"pattern\", layer].squeeze()\n",
    "        \n",
    "    for head in range(num_heads):\n",
    " \n",
    "        attention_to_final_token = attention_patterns[head, :, final_token_index]\n",
    "            \n",
    "        \n",
    "        avg_attention = attention_to_final_token.mean()\n",
    "            \n",
    "           \n",
    "        avg_attention_matrix[head, layer] = avg_attention\n",
    "\n",
    "   \n",
    "plot_data = avg_attention_matrix.detach().cpu().numpy()\n",
    "\n",
    "    # Create the heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(\n",
    "        plot_data,\n",
    "        cmap=\"viridis\",\n",
    "        linewidths=0.5,\n",
    "        xticklabels=[f'L{i}' for i in range(num_layers)],\n",
    "        yticklabels=[f'H{i}' for i in range(num_heads)],\n",
    "        cbar_kws={'label': 'Average Attention to Final Token'}\n",
    "    )\n",
    "plt.title(\"Attention Weights on Final Token (All Layers and Heads)\")\n",
    "plt.xlabel(\"Layer Number\")\n",
    "plt.ylabel(\"Head Number\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5595d-40d8-426e-bc78-dd457fcbd9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_number = 10\n",
    "\n",
    "attention_pattern_all_heads = cache[\"pattern\", layer_number].squeeze().detach().cpu().numpy()\n",
    "num_heads = attention_pattern_all_heads.shape[0]\n",
    "\n",
    "input_tokens = tokenizer.tokenize(text_to_analyze)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "        nrows=num_heads // 8, \n",
    "        ncols=8, \n",
    "        figsize=(20, 10), \n",
    "        constrained_layout=True\n",
    "    )\n",
    "    \n",
    "  \n",
    "axes = axes.flatten()\n",
    "    \n",
    "for i in range(num_heads):\n",
    "    ax = axes[i]\n",
    "    sns.heatmap(\n",
    "        attention_pattern_all_heads[i], \n",
    "        xticklabels=input_tokens, \n",
    "        yticklabels=input_tokens,\n",
    "        cmap=\"viridis\",\n",
    "        linewidths=0.2,\n",
    "        ax=ax,\n",
    "        cbar=False\n",
    "        )\n",
    "    ax.set_title(f'Head {i}')\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    ax.tick_params(axis='y', rotation=0)\n",
    "\n",
    "\n",
    "fig.suptitle(f\"Attention Heatmaps for Layer {layer_number}\", fontsize=20)\n",
    "fig.supxlabel(\"Key Tokens (Input)\", fontsize=12)\n",
    "fig.supylabel(\"Query Tokens (Output)\", fontsize=12)\n",
    "    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d0471-2e7b-492f-a768-3a88fa4d8179",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "head = 1\n",
    "\n",
    "attention_pattern = cache[\"pattern\", layer]\n",
    "\n",
    "print(attention_pattern.shape)\n",
    "    \n",
    "\n",
    "\n",
    "attention_to_plot = attention_pattern.squeeze().detach().cpu().numpy()[head]\n",
    "    \n",
    "\n",
    "input_tokens = tokenizer.tokenize(text_to_analyze)\n",
    "    \n",
    "print(f\"\\n Visualizing Attention Heatmap for Layer {layer}, Head {head} ---\")\n",
    "    \n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_to_plot, \n",
    "            xticklabels=input_tokens, \n",
    "            yticklabels=input_tokens,\n",
    "            cmap=\"viridis\",\n",
    "            linewidths=0.5)\n",
    "plt.title(f\"Attention Heatmap: Layer {layer}, Head {head}\")\n",
    "plt.xlabel(\"Key Tokens (Input)\")\n",
    "plt.ylabel(\"Query Tokens (Output)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fcc417-3343-4989-a612-656293f430fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer = 17\n",
    "head = 14\n",
    "\n",
    "attention_pattern = cache[\"pattern\", layer]\n",
    "\n",
    "print(attention_pattern.shape)\n",
    "    \n",
    "\n",
    "\n",
    "attention_to_plot = attention_pattern.squeeze().detach().cpu().numpy()[head]\n",
    "    \n",
    "\n",
    "input_tokens = tokenizer.tokenize(text_to_analyze)\n",
    "    \n",
    "print(f\"\\n Visualizing Attention Heatmap for Layer {layer}, Head {head} ---\")\n",
    "    \n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_to_plot, \n",
    "            xticklabels=input_tokens, \n",
    "            yticklabels=input_tokens,\n",
    "            cmap=\"viridis\",\n",
    "            linewidths=0.5)\n",
    "plt.title(f\"Attention Heatmap: Layer {layer}, Head {head}\")\n",
    "plt.xlabel(\"Key Tokens (Input)\")\n",
    "plt.ylabel(\"Query Tokens (Output)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a74308f-2bd0-4d8a-aa46-8a4b574fb204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer = 21\n",
    "head = 24\n",
    "\n",
    "attention_pattern = cache[\"pattern\", layer]\n",
    "\n",
    "print(attention_pattern.shape)\n",
    "    \n",
    "\n",
    "attention_to_plot = attention_pattern.squeeze().detach().cpu().numpy()[head]\n",
    "    \n",
    "\n",
    "input_tokens = tokenizer.tokenize(text_to_analyze)\n",
    "    \n",
    "print(f\"\\n Visualizing Attention Heatmap for Layer {layer}, Head {head} ---\")\n",
    "    \n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_to_plot, \n",
    "            xticklabels=input_tokens, \n",
    "            yticklabels=input_tokens,\n",
    "            cmap=\"viridis\",\n",
    "            linewidths=0.5)\n",
    "plt.title(f\"Attention Heatmap: Layer {layer}, Head {head}\")\n",
    "plt.xlabel(\"Key Tokens (Input)\")\n",
    "plt.ylabel(\"Query Tokens (Output)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706c852e-38d2-488f-8e02-eaae95537646",
   "metadata": {},
   "source": [
    "## Causal Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e040bd-089f-440e-a8fc-4dc28d17f756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_model_embedding_layer(model):\n",
    "    \"\"\"\n",
    "    Finds and returns the model's token embedding layer, handling various attribute names.\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'get_input_embeddings'):\n",
    "        return model.get_input_embeddings\n",
    "    elif hasattr(model, 'embed'):\n",
    "        return model.embed\n",
    "    elif hasattr(model, 'w_e'):\n",
    "        return model.w_e\n",
    "    else:\n",
    "        raise AttributeError(\"Could not find the model's embedding layer or a valid embedding method (checked for 'get_input_embeddings', 'embed', and 'w_e').\")\n",
    "    \n",
    "    \n",
    "# --- Functions for calculating sigma and embeddings ---\n",
    "\n",
    "def calculate_calibration_stats(model, tokenizer, prompts: List[str]) -> Tuple[float, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Calculates sigma (3 * empirical std dev) and the mean embedding of a set of prompts.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process each prompt to get its embedding\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if hasattr(model, 'get_input_embeddings'):\n",
    "                token_embeddings = model.get_input_embeddings()(input_ids)\n",
    "            elif hasattr(model, 'embed'):\n",
    "                token_embeddings = model.embed(input_ids)\n",
    "            else:\n",
    "                raise AttributeError(\"Could not find the model's embedding layer or a valid embedding method.\")\n",
    "        \n",
    "        mean_embedding = torch.mean(token_embeddings, dim=1)\n",
    "        all_embeddings.append(mean_embedding.squeeze())\n",
    "        \n",
    "    stacked_embeddings = torch.stack(all_embeddings)\n",
    "\n",
    "    # Calculate the standard deviation and mean of all embeddings\n",
    "    empirical_std_dev = torch.std(stacked_embeddings, dim=0, unbiased=False)\n",
    "    mean_embedding = torch.mean(stacked_embeddings, dim=0)\n",
    "\n",
    "    # Calculate sigma as 3 times the mean of the std dev vector\n",
    "    sigma = 3 * torch.mean(empirical_std_dev).item()\n",
    "    \n",
    "    return sigma, mean_embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15238ea3-2ca3-4484-a287-627a6a1f3904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calibration_prompts = hallu_df['constrained prompt'].tolist()\n",
    "\n",
    "sigma_value, mean_embedding_vector = calculate_calibration_stats(model_llama, tokenizer, calibration_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93f240-d39f-4247-ac45-e6c8462f5a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Sigma:\", sigma_value)\n",
    "mean_embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc434f-b42c-47dc-8684-54c97f847e48",
   "metadata": {},
   "source": [
    "## Hypothesis on adding noises to which embedding tokens as to why the model is hallucinating.\n",
    "\n",
    "2 main options, each testing a different hypothesis:\n",
    "\n",
    "1. Add Noise to the Subject Token ('i')\n",
    "Hypothesis: The hallucination (predicting \"joy\" instead of \"sadness\") is caused by a misinterpretation of the speaker's state. The model incorrectly represents the speaker's emotional state, which is primarily anchored to the token 'i'.\n",
    "\n",
    "Inject noise only into the embedding of the first token, 'i', and observe if the model's prediction changes.\n",
    "\n",
    "2. Add Noise to the Key Emotional Tokens ('shame' and 'stigma')\n",
    "Hypothesis: The hallucination is caused by the model misinterpreting emotionally charged words in the text.\n",
    "\n",
    "Inject noise only into the embeddings of the tokens for \"shame\" and \"stigma.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f289eb-2138-443a-896c-ad0f53d93067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_subject_token(tokenizer, prompt):\n",
    "    \"\"\"\n",
    "    Finds the first token of a prompt, assuming it represents the subject.\n",
    "    \"\"\"\n",
    "    # 1. Clean the prompt by stripping leading/trailing quotes and spaces\n",
    "    clean_prompt = prompt.strip(' \\'\"')\n",
    "    \n",
    "    # 2. Use the tokenizer to get the token IDs for the cleaned prompt\n",
    "    input_ids = tokenizer.encode(clean_prompt, add_special_tokens=False)\n",
    "    \n",
    "    # Handle empty prompts after cleaning\n",
    "    if not input_ids:\n",
    "        return \"\"\n",
    "        \n",
    "    # 3. Get the ID of the first token\n",
    "    first_token_id = input_ids[0]\n",
    "    \n",
    "    # 4. Decode the ID back to a string to get the token text\n",
    "    first_token_text = tokenizer.decode(first_token_id)\n",
    "    \n",
    "    # 5. Return the first token text, stripping any leading spaces\n",
    "    return first_token_text.strip()\n",
    "\n",
    "# Example usage with your provided prompt from the DataFrame\n",
    "prompt_from_df = hallu_df['constrained prompt'].iloc[59]\n",
    "\n",
    "# The function will now handle the cleaning automatically\n",
    "subject_text = find_subject_token(tokenizer, prompt_from_df)\n",
    "print(f\"The subject token is: '{subject_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc32cc9-0bbe-43ae-a6e2-9ad3193f54a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " hallu_df['constrained prompt'].iloc[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479fed7-b0e6-4aa3-b036-1faaf0e5f6b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_perturbed_embeddings(model, tokenizer, prompt, sigma):\n",
    "    \"\"\"\n",
    "    Calculates u* by adding scaled Gaussian noise to the embedding of the\n",
    "    first token in the prompt, which is assumed to be the subject.\n",
    "\n",
    "    Args:\n",
    "        model: The loaded TransformerLens model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        prompt (str): The original prompt text.\n",
    "        sigma (float): The calculated standard deviation for the noise.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The perturbed input embeddings (u*).\n",
    "    \"\"\"\n",
    "    # 1. Cleaning the prompt by stripping leading/trailing quotes and spaces\n",
    "    clean_prompt = prompt.strip(' \\'\"')\n",
    "    input_ids = tokenizer.encode(clean_prompt, return_tensors='pt')\n",
    "    \n",
    "    if input_ids.numel() == 0:\n",
    "        print(\"Warning: Prompt is empty after cleaning. Cannot generate embeddings.\")\n",
    "        return None\n",
    "\n",
    "    # Getting the original embeddings from the model's embedding layer\n",
    "    with torch.no_grad():\n",
    "        original_embeddings = model.embed(input_ids)\n",
    "        \n",
    "   \n",
    "    subject_token_index = 0\n",
    "    \n",
    "    # 2. Creating Gaussian noise scaled by sigma\n",
    "    noise = torch.randn_like(original_embeddings) * sigma\n",
    "    \n",
    "    # 3. Creating a new tensor for u* by copying the original embeddings\n",
    "    u_star_embeddings = original_embeddings.clone()\n",
    "    \n",
    "    # 4. Injecting the noise into the subject token's embedding\n",
    "    u_star_embeddings[0, subject_token_index, :] += noise[0, subject_token_index, :]\n",
    "    \n",
    "    return u_star_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c73dd-9194-4cbf-a088-9d8a00390d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u_star = get_perturbed_embeddings(model_llama, tokenizer,  hallu_df['constrained prompt'].iloc[55], sigma_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a14ac0-6aec-44fb-88d3-552055093cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u_star.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727eafd8-8781-46bf-ad18-441d70fa1bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u_star_tensors = {}  # This dictionary will store all your u* tensors\n",
    "\n",
    "# Loop through each row of your hallucination DataFrame\n",
    "for index, row in hallu_set1_df.iterrows():\n",
    "    prompt = row['constrained prompt']\n",
    "    \n",
    "    # Calculate u* for the current prompt\n",
    "    perturbed_embeddings = get_perturbed_embeddings(model_llama, tokenizer, prompt, sigma_value)\n",
    "    \n",
    "    # Check if the embeddings were successfully generated\n",
    "    if perturbed_embeddings is not None:\n",
    "        # Store the tensor in the dictionary with the prompt's index as the key\n",
    "        u_star_tensors[index] = perturbed_embeddings\n",
    "\n",
    "print(f\"Stored {len(u_star_tensors)} u* tensors for analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec76b26a-d658-4490-b064-b386b27a9a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def perform_causal_analysis_y_prime(model, tokenizer, hallu_df, sigma_value, num_noise_samples):\n",
    "    \"\"\"\n",
    "    Performs a more memory-efficient causal analysis by optimizing\n",
    "    the generation of noise samples.\n",
    "    \"\"\"\n",
    "    results_df = pd.DataFrame(columns=['prompt_text', 'true_emotion', 'predicted_emotion', 'num_truth_inducing_samples', 'truthful_y_primes'])\n",
    "\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for index, row in hallu_df.iterrows():\n",
    "            prompt_to_analyze = row['constrained prompt'].strip()\n",
    "            true_emotion = row['emotion']\n",
    "            predicted_emotion = row['predicted emotion']\n",
    "            \n",
    "            \n",
    "            truthful_y_primes = []\n",
    "            \n",
    "            # 1. Running the original prompt once to get the original embeddings\n",
    "            _, original_cache = model.run_with_cache(prompt_to_analyze)\n",
    "            original_embeddings = original_cache['embed'].clone().detach()\n",
    "            \n",
    "            # clearing the cache to free up memory before the noise sampling loop starts\n",
    "            del original_cache\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # 2. token IDs for the true and predicted emotions once\n",
    "            try:\n",
    "                true_id = tokenizer.encode(true_emotion, add_special_tokens=False)[0]\n",
    "                predicted_id = tokenizer.encode(predicted_emotion, add_special_tokens=False)[0]\n",
    "            except IndexError:\n",
    "                print(f\"Skipping prompt {index}: Emotion token not found.\")\n",
    "                continue\n",
    "\n",
    "            # 3. Using a single noise tensor for all samples to minimize memory allocation\n",
    "            \n",
    "            \n",
    "            for _ in range(num_noise_samples):\n",
    "                # Adding noise to the original embeddings to get u*\n",
    "            \n",
    "                subject_token_index = 0\n",
    "                noise = torch.randn_like(original_embeddings) * sigma_value\n",
    "                perturbed_embeddings = original_embeddings.clone()\n",
    "                perturbed_embeddings[0, subject_token_index, :] += noise[0, subject_token_index, :]\n",
    "                \n",
    "                # Using a hook to replace the 'embed' output with the perturbed embeddings\n",
    "                def hook_fn_replace_embed(embed_output, hook):\n",
    "                    return perturbed_embeddings\n",
    "                \n",
    "                # Running the model with the hook\n",
    "                new_logits = model.run_with_hooks(\n",
    "                    input=tokenizer.encode(prompt_to_analyze, return_tensors='pt'),\n",
    "                    fwd_hooks=[('hook_embed', hook_fn_replace_embed)]\n",
    "                )\n",
    "                \n",
    "                # Computing the new log-likelihood ratio (y') from the new logits\n",
    "                final_logits = new_logits[0, -1, :]\n",
    "                y_prime = final_logits[predicted_id] - final_logits[true_id]\n",
    "                \n",
    "                # Filtering for \"truth-inducing\" samples\n",
    "                if y_prime.item() < 1:\n",
    "                    truthful_y_primes.append(y_prime.item())\n",
    "\n",
    "            \n",
    "            new_row = pd.DataFrame([{\n",
    "                'prompt_text': prompt_to_analyze,\n",
    "                'true_emotion': true_emotion,\n",
    "                'predicted_emotion': predicted_emotion,\n",
    "                'num_truth_inducing_samples': len(truthful_y_primes),\n",
    "                'truthful_y_primes': truthful_y_primes\n",
    "            }])\n",
    "            results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "            \n",
    "            # free memory after each prompt\n",
    "            del original_embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad929965-df76-4152-b829-de0e6a6d45b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "y_prime1 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set1_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c9c30-b8c1-4ca5-a8d3-f537af96fbee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dddc62c-a5c0-4c69-9689-b0fa43c6cae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime1[(y_prime1['num_truth_inducing_samples']>50)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d42162fa-9f40-4b15-88db-61106b27ee8d",
   "metadata": {},
   "source": [
    "The results from causal analysis provide a clear distinction between different types of hallucinations: brittle and robust.\n",
    "\n",
    "Brittle Hallucination: The hallucination is \"brittle\" if the model can be easily nudged back towards the correct answer. This happens when the true emotion's knowledge is present internally but is overshadowed by a stronger, albeit incorrect, signal at the final layer.\n",
    "\n",
    "Robust Hallucination: The hallucination is \"robust\" if adding noise does not correct the model's output. This suggests the error is deeply ingrained and not easily corrected by a simple intervention at the embedding layer. The model may have genuinely lost or mislearned the information needed for the correct output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac66dd-b791-4225-9846-bd32b53cc425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime2 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set2_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f444a4c-ae5e-4fd7-ac4e-fd95eeada999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c908aa-dae0-45e8-8fed-d9146456b876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime3 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set3_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d5fb5-db89-4fa7-a54d-ee9dff3ccd18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095600a-664f-4bdc-95cc-7175ee3196ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime4 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set4_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d864f41-5a7f-45d7-842d-f251a75f9660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e99b1-7f43-452f-9daa-b94167dbb2b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime5 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set5_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3931ee1f-4469-48bd-a9c6-937ff949878a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77837b-e472-4ed0-b1e9-fd4b74c51246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime6 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set6_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b343e57-006d-4468-9801-1b2a66137cbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cfe399-5afa-4904-806a-1f755d9e79fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime7 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set7_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e591dea-2dff-408e-8aa6-fac4abc3b319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f7822-2def-4da9-8bef-238bccb09aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime8 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set8_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5da791-8ba2-4631-99c6-3e91be10f25d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dde573a-ee50-40e7-86ca-b96e4338634b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime9 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set9_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43841ef0-ab1f-4fe4-ba82-f5ef453d1c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c65eb-9581-42f9-8a6e-ddaabb0d6e11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime10 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set10_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74914226-a8d0-4e50-bef7-deee77437013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724926a-12aa-4764-ab50-603462a6ca03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime11 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set10_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9650b-f23e-43e1-b693-8482e64d2fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68faec1-712a-4556-834f-df1aa8ec2f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime12 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set10_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b96bba5-31f5-41c2-9e5b-1db91734c43b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f3816-a006-48e1-94f7-700d8648494f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime13 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set10_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625264d4-61a2-49be-9be7-2808698dd399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93210e4-5e22-4a72-8b41-1cf553211a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime14 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set10_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0190c76c-99d4-4abe-951b-05132d7185fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a5b539-90fb-4888-b0bf-b71cc0c8326c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime15 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set10_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217490f-7018-4892-b389-8b3b08fff5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22001099-5fb5-4ca4-a34b-98e3ae1e4a02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime16 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set10_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6725af72-1c14-471b-8636-975e5f2bbaf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f9705-ba03-4de8-ad0c-17491209aa90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime17 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set10_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f469c2-c29a-4d11-b40a-cbdbd7bd326a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d91333-242f-4ebc-8f89-601fea7572cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime18 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set10_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfd74d-2c85-4c46-a454-208cbd7a157b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011a1e2-6c6e-4209-a417-488034bbe7a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime19 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set10_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900defc-c655-41c5-a417-c0dac0151e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8590e4f-19ec-446a-86ca-e391f68e05b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "y_prime20 = perform_causal_analysis_y_prime(model_llama, tokenizer, hallu_set10_df, sigma_value, num_noise_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59399b4-5b9a-4e67-bb8f-3805f653da37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1d09e-9cbe-4fc0-bb7a-b77b7c7fb916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_target_token_indices(tokenizer, prompt, target_tokens):\n",
    "    \"\"\"\n",
    "    Finds the token indices for all occurrences of the target tokens in a prompt,\n",
    "    handling sub-word tokenization by checking for substring matches.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        prompt (str): The prompt text to analyze.\n",
    "        target_tokens (list): A list of lowercase strings representing the target words.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of integers corresponding to the indices of the target tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt and get a list of the decoded tokens\n",
    "    input_ids = tokenizer.encode(prompt.strip(), add_special_tokens=False)\n",
    "    decoded_tokens = [tokenizer.decode(token_id).strip().lower() for token_id in input_ids]\n",
    "    \n",
    "    target_indices = []\n",
    "    \n",
    "    for i, token in enumerate(decoded_tokens):\n",
    "        for target_word in target_tokens:\n",
    "            # Check for both a full word match and if the token is a part of a target word\n",
    "            if token == target_word or target_word in token:\n",
    "                if i not in target_indices: # Ensure we only add each index once\n",
    "                    target_indices.append(i)\n",
    "                \n",
    "    return target_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555ed1f-2712-4ead-bf1b-191682c29321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_emotional_tokens = ['abandoned', 'abandonment', 'abhor', 'abhorrent', 'aberration', 'abduction', 'ability', 'abba', \n",
    "                           'abbot', 'affecting', 'affective', 'affiliate', 'affirm', 'affirmation', 'affirmative', 'afflict', \n",
    "                           'afflicted', 'affliction', 'affluence', 'afford', 'affront', 'afraid', 'afresh', 'aftermath', 'agape', \n",
    "                           'aggressor', 'aggravate', 'aggravating', 'aggression', 'aggressive', 'aggressiveness', 'aghast', 'agitate', \n",
    "                           'agitated', 'agitation', 'agony', 'agree', 'agreeable', 'agreed', 'agreement', 'alarm', 'alarming', 'albatross', \n",
    "                           'alert', 'alienate', 'alienated', 'alienation', 'alive', 'allegiance', 'alleviate', 'alone', 'amazement', 'amazed', \n",
    "                           'amazing', 'ambition', 'ambitious', 'ameliorate', 'amusement', 'amusing', 'anarchism', 'anarchist', 'anarchy', 'angel', \n",
    "                           'angelic', 'anger', 'angry', 'anguish', 'animosity', 'annoy', 'annoyance', 'annoying', 'anomaly', 'antagonism', 'antagonist',\n",
    "                           'antagonistic', 'anxiety', 'anxious', 'apathetic', 'apathy', 'apologize', 'apology', 'appeal', 'appealing', 'appease', 'applaud', \n",
    "                           'applause', 'appreciation', 'appreciative', 'apprehensive', 'apprehension', 'approved', 'arrogance', 'arrogant', 'ashamed', 'assault',\n",
    "                           'assassination', 'assure', 'assured', 'astonished', 'astonishing', 'astonishment', 'atrocious', 'atrocity', 'aversion', 'avert', 'avid',\n",
    "                           'awful', 'awkward', 'backfire', 'bad', 'baffle', 'baffled', 'bafflement', 'baleful', 'ballyhoo', 'banter', 'baseless', 'beautiful', \n",
    "                           'beauty', 'beg', 'beguile', 'belittle', 'belligerence', 'belligerent', 'benefit', 'beneficial', 'benevolence', 'benevolent', 'bereaved',\n",
    "                           'bereavement', 'bestial', 'betray', 'betrayal', 'bicker', 'bickering', 'bitter', 'bitterness', 'blackmail', 'blame', 'blight', 'bliss',\n",
    "                           'blissful', 'boast', 'boredom', 'bother', 'brave', 'bravery', 'breakthrough', 'bribery', 'bright', 'brilliant', 'brutal', 'brutality',\n",
    "                           'bully', 'calamity', 'calm', 'calmness', 'canonize', 'captivate', 'care', 'careful', 'carefully', 'caress', 'catastrophe', 'celebrate',\n",
    "                           'celebration', 'certain', 'certainty', 'chagrin', 'charity', 'charm', 'charming', 'cheer', 'cheerful', 'cheerless', 'cheery', 'cherish',\n",
    "                           'clash', 'coerce', 'coercion', 'collapse', 'comfort', 'comfortable', 'compasionate', 'compassion', 'complacent', 'complaint', 'compliment',\n",
    "                           'complimentary', 'composure', 'confess', 'confession', 'confidence', 'confident', 'conflict', 'confound', 'confrontation', 'confuse', \n",
    "                           'confused', 'confusion', 'congratulate', 'congratulation', 'conspiracy', 'contagion', 'contagious', 'contempt', 'contemptible', \n",
    "                           'contemptuous', 'contentment', 'corrupt', 'corruption', 'courage', 'courageous', 'covetous', 'cower', 'cramped', 'crash', 'crave', \n",
    "                           'craving', 'crazed', 'crazy', 'crush', 'crying', 'cynical', 'damage', 'daring', 'darkness', 'dastardly', 'dauntless', 'dead', 'dear',\n",
    "                           'dearth', 'death', 'decay', 'deceit', 'deceitful', 'deceive', 'deception', 'defeat', 'defeated', 'defect', 'defective', 'defense',\n",
    "                           'defenseless', 'defer', 'defiance', 'deficient', 'deformity', 'dejected', 'dejection', 'delay', 'delight', 'delighted', 'demise', \n",
    "                           'demolition', 'demon', 'denounce', 'denunciation', 'deny', 'deplete', 'deplorable', 'depravity', 'depressed', 'depression', 'deprive', \n",
    "                           'deprived', 'deserve', 'desirable', 'desire', 'despair', 'desperate', 'desperation', 'despicable', 'despise', 'destroy', 'destruction', \n",
    "                           'despondent', 'despondency', 'detest', 'detestable', 'devastate', 'devastating', 'devil', 'devilish', 'devotion', 'devout', 'difficult', \n",
    "                           'difficulty', 'disagree', 'disagreement', 'disappointed', 'disappointment', 'disapprove', 'disapproval', 'disaster', 'disastrous',\n",
    "                           'disbelief', 'discomfort', 'discontent', 'discontented', 'discord', 'discourage', 'disgust', 'disgusted', 'disgusting', 'dishearten', \n",
    "                           'disheveled', 'dishonest', 'dishonesty', 'dishonor', 'disintegrate', 'dislike', 'dismal', 'dismay', 'dismayed', 'displeased', \n",
    "                           'displeasure', 'disrespect', 'disrespectful', 'distress', 'distressed', 'distrust', 'disturb', 'divorce', 'dominant', 'doubt',\n",
    "                           'dread', 'dreadful', 'dull', 'eager', 'ecstasy', 'elation', 'eloquence', \n",
    "                           'embrace', 'emotional', 'empty', 'enchantment', 'encouragement', 'endearing', 'enjoy', 'enjoyment', 'enrage', 'enraged', 'entice',\n",
    "                           'entitlement', 'enthusiasm', 'enthusiastic', 'envy', 'evil', 'exasperation', 'excite', 'excited', 'excitement', 'exhausted', 'exuberant',\n",
    "                           'fabulous', 'faint', 'faith', 'faithful', 'falsify', 'famish', 'fanatic', 'fantastic', 'fear', 'fearful', 'fearless', 'fearsome', \n",
    "                           'feeble', 'felicity', 'ferocious', 'fiery', 'fight', 'filthy', 'flagging', 'flatter', 'flattery', 'flustered', 'foe', 'fondness', \n",
    "                           'fool', 'foolish', 'forgive', 'forgiveness', 'forlorn', 'fortunate', 'fortune', 'foul', 'frantic', 'frenzy', 'friend', 'friendly', \n",
    "                           'fright', 'frightened', 'frightening', 'frown', 'frustrate', 'frustrated', 'frustration', 'furious', 'fury', 'gallant', 'gaudy', \n",
    "                           'ghastly', 'giddy', 'gloom', 'gloomy', 'glorious', 'glory', 'grief', 'grieving', 'grimace', 'gross', 'grotesque', 'guilt', 'guilty',\n",
    "                           'hapless', 'happiness', 'happy', 'harass', 'harm', 'harsh', 'hatred', 'haunt', 'haunted', 'heartbreak', 'heartbreaking', 'heartless',\n",
    "                           'heaven', 'hell', 'helpless', 'hesitation', 'hideous', 'hope', 'hopeful', 'hopeless', 'hopelessness', 'horrendous', 'horrible', 'horror',\n",
    "                           'humiliate', 'humiliation', 'hurt', 'hymn', 'idiot', 'ignorant', 'ill', 'illness', 'imbecile', 'immortal', 'impatient', 'important', \n",
    "                           'inability', 'inadequate', 'incensed', 'incite', 'indignant', 'indignation', 'infatuated', 'infatuation', 'infection', 'inferior',\n",
    "                           'inferno', 'infuriate', 'infuriated', 'infuriating', 'insane', 'insanity', 'insidious', 'insult', 'insulting', 'integrity', 'interest',\n",
    "                           'interested', 'invasion', 'invigorate', 'involve', 'irk', 'jealousy', 'jest', 'jinx', 'jovial', 'joy', 'joyful', 'jubilation', 'karma', \n",
    "                           'kidnap', 'kindness', 'lack', 'lament', 'lamentation', 'laugh', 'laughter', 'leisure', 'liberation', 'liberty', 'lie', 'light', 'like',\n",
    "                           'love', 'loyal', 'loyalty', 'ludicrous', 'lust', 'maddening', 'madness', 'malaise', 'malice', 'malignant', 'maniac', 'massacre', \n",
    "                           'menace', 'merciless', 'mercy', 'mirth', 'misery', 'mishap', 'mistake', 'molestation', 'monstrous', 'morbid', 'mourn', 'mournful', \n",
    "                           'murder', 'mutiny', 'nasty', 'nausea', 'nauseating', 'neglect', 'neglected', 'neglectful', 'nerve', 'nervous', 'nightmare',\n",
    "                           'obnoxious', 'obscene', 'offend', 'offense', 'ominous', 'optimism', 'optimistic', 'outrage', 'outrageous', 'overjoy',\n",
    "                           'panic', 'paradise', 'passion', 'patience', 'peace', 'peaceful', 'pessimistic', 'pity', 'plague', 'pleasant', 'pleasure',\n",
    "                           'poison', 'poisonous', 'praise', 'pride', 'promising', 'protest', 'proud', 'rage', 'rape', 'rapport', 'rascal', 'relieve', \n",
    "                           'relief', 'remorse', 'remorseful', 'resentment', 'respect', 'revenge', 'revulsion', 'ridiculous', 'rigid', 'risk', 'sadness', \n",
    "                           'safe', 'safety', 'salvation', 'sanguine', 'sarcasm', 'savage', 'scare', 'scared', 'scary', 'scream', 'screaming', 'screech', \n",
    "                           'secure', 'sensational', 'sensitive', 'serene', 'sham', 'shame', 'shattered', 'shock', 'shocking', 'shriek', 'sick', 'sickness', \n",
    "                           'sincere', 'sincerity', 'sneer', 'solemn', 'sorrow', 'sorrowful', 'spectacular', 'splendid', 'squalor', 'stab', 'startle', 'startling',\n",
    "                           'strangle', 'stupid', 'suffering', 'suffocate', 'superb', 'surprise', 'surprised', 'suspense', 'suspicious', 'swindle', 'sympathy', \n",
    "                           'terror', 'terrible', 'terrific', 'terrified', 'threat', 'threaten', 'thrilled', 'thrilling', 'tragedy', 'tragic', 'triumph',\n",
    "                           'triumphant', 'trust', 'trusted', 'trusting', 'ugly', 'uncomfortable', 'unhappiness', 'unhappy', 'uninspired', 'unpleasant', \n",
    "                           'upset', 'upsetting', 'vengeance', 'vicious', 'victory', 'violent', 'want', 'wary', 'weak', 'weakness', 'weep', 'weeping', 'welcome',\n",
    "                           'woe', 'wonderful', 'worry', 'wretched', 'wrong', 'wrongdoing', 'yearning', 'yell', 'zest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad570a-771f-4fdb-9c7a-4aaa2ecec388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_causal_analysis_emotional_tokens(model, tokenizer, prompt_df, sigma_value, num_noise_samples, target_tokens):\n",
    "    \"\"\"\n",
    "    Performs causal analysis by adding scaled Gaussian noise to the embeddings of\n",
    "    emotionally charged tokens, testing if this intervention reduces the hallucination.\n",
    "\n",
    "    Args:\n",
    "        model (HookedTransformer): The loaded TransformerLens model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        hallu_df (pd.DataFrame): DataFrame containing hallucinated prompts.\n",
    "        sigma_value (float): The standard deviation for the Gaussian noise.\n",
    "        num_noise_samples (int): The number of noise samples to test per prompt.\n",
    "        target_tokens (list): A list of emotionally charged words to target for noise injection.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the results of the analysis, including\n",
    "                      the number of \"truth-inducing\" samples and their y' values.\n",
    "    \"\"\"\n",
    "    results_df = pd.DataFrame(columns=['prompt_text', 'true_emotion', 'predicted_emotion', 'num_truth_inducing_samples', 'truthful_y_primes'])\n",
    "\n",
    "    # Ensure no gradients are computed to save memory and computation\n",
    "    with torch.no_grad():\n",
    "        for index, row in prompt_df.iterrows():\n",
    "            prompt_to_analyze = row['constrained prompt']\n",
    "            true_emotion = row['emotion']\n",
    "            predicted_emotion = row['predicted emotion']\n",
    "            \n",
    "            # Find all indices of the target emotional tokens\n",
    "            target_indices = get_target_token_indices(tokenizer, prompt_to_analyze, target_tokens)\n",
    "            \n",
    "            # Skip if no target tokens are found in this prompt\n",
    "            if not target_indices:\n",
    "                print(f\"Skipping prompt {index}: No target tokens found.\")\n",
    "                continue\n",
    "\n",
    "            truthful_y_primes = []\n",
    "            \n",
    "            # 1. Running the original prompt once to get the original embeddings\n",
    "            _, original_cache = model.run_with_cache(prompt_to_analyze.strip())\n",
    "            original_embeddings = original_cache['embed'].clone().detach()\n",
    "            \n",
    "            # Clear the cache to free up memory before the noise sampling loop starts\n",
    "            del original_cache\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # 2. Get token IDs for the true and predicted emotions once\n",
    "            try:\n",
    "                # Some tokenizers prepend a space, so we check for both cases\n",
    "                true_id_with_space = tokenizer.encode(\" \" + true_emotion, add_special_tokens=False)[0]\n",
    "                predicted_id_with_space = tokenizer.encode(\" \" + predicted_emotion, add_special_tokens=False)[0]\n",
    "                true_id_no_space = tokenizer.encode(true_emotion, add_special_tokens=False)[0]\n",
    "                predicted_id_no_space = tokenizer.encode(predicted_emotion, add_special_tokens=False)[0]\n",
    "            except IndexError:\n",
    "                print(f\"Skipping prompt {index}: Emotion token not found.\")\n",
    "                continue\n",
    "            \n",
    "            for _ in range(num_noise_samples):\n",
    "                # Add noise to the original embeddings to get u*\n",
    "                noise = torch.randn_like(original_embeddings) * sigma_value\n",
    "                perturbed_embeddings = original_embeddings.clone()\n",
    "\n",
    "                # Iterate through the found indices and add noise to each one\n",
    "                for token_idx in target_indices:\n",
    "                    perturbed_embeddings[0, token_idx, :] += noise[0, token_idx, :]\n",
    "                \n",
    "                # Using a hook to replace the 'embed' output with the perturbed embeddings\n",
    "                def hook_fn_replace_embed(embed_output, hook):\n",
    "                    return perturbed_embeddings\n",
    "                \n",
    "                # Running the model with the hook\n",
    "                new_logits = model.run_with_hooks(\n",
    "                    input=tokenizer.encode(prompt_to_analyze.strip(), return_tensors='pt'),\n",
    "                    fwd_hooks=[('hook_embed', hook_fn_replace_embed)]\n",
    "                )\n",
    "                \n",
    "                # Computing the new log-likelihood ratio (y') from the new logits\n",
    "                final_logits = new_logits[0, -1, :]\n",
    "                \n",
    "                # We try both possible token IDs to be safe\n",
    "                if (predicted_id_with_space in final_logits and true_id_with_space in final_logits):\n",
    "                    y_prime = final_logits[predicted_id_with_space] - final_logits[true_id_with_space]\n",
    "                elif (predicted_id_no_space in final_logits and true_id_no_space in final_logits):\n",
    "                    y_prime = final_logits[predicted_id_no_space] - final_logits[true_id_no_space]\n",
    "                else:\n",
    "                    # If neither token is in the vocab, we skip this sample\n",
    "                    continue\n",
    "                \n",
    "                # Filtering for \"truth-inducing\" samples where y' < 1\n",
    "                if y_prime.item() < 1:\n",
    "                    truthful_y_primes.append(y_prime.item())\n",
    "\n",
    "            new_row = pd.DataFrame([{\n",
    "                'prompt_text': prompt_to_analyze,\n",
    "                'true_emotion': true_emotion,\n",
    "                'predicted_emotion': predicted_emotion,\n",
    "                'num_truth_inducing_samples': len(truthful_y_primes),\n",
    "                'truthful_y_primes': truthful_y_primes\n",
    "            }])\n",
    "            results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "            \n",
    "            # Free memory after each prompt\n",
    "            del original_embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d87c45-76a2-48c6-ae12-02557e32e5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime1_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set1_df, sigma_value, 100, target_emotional_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c70b6-bdcb-4c4c-abdf-4926a7444bed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime1_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75808866-2a55-4eb1-8d3c-25038f207547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime1_H2[(y_prime1_H2['num_truth_inducing_samples']>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ff9c2-08ab-408e-a671-afdf5ea2f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime2_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set2_df, sigma_value, 100, target_emotional_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3745757c-3fda-4a9b-83e6-4b56d323b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime2_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ac9d50-b4b3-4099-8942-b30a70c5279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime3_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set3_df, sigma_value, 100, target_emotional_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c610ea9-cf24-4e96-9350-4bdbdaaf9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime3_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef1795-d0f5-49bc-b9d2-417576ee6b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime4_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set4_df, sigma_value, 100, target_emotional_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176ffa5-ae7c-476c-833f-9c1e5979d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime4_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9112599-e2ec-4b47-8b76-970e54061e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime5_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set5_df, sigma_value, 100, target_emotional_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c2240-db29-458f-9c61-8afaf8c0ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime5_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb08b1-2159-448e-8d98-115e8bd6b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime6_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set6_df, sigma_value, 100, target_emotional_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0715f80-db94-4019-9200-0ac32c865e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime6_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388f106-2272-4442-981a-bbac41b2a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime7_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set7_df, sigma_value, 100, target_emotional_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c3e34-5b20-473c-8f75-130ab3074360",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime7_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafdc14c-690b-4b63-ac0b-ca1c699c485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime8_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set8_df, sigma_value, 100, target_emotional_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb76e905-e1b0-44d1-a582-aa028d432a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime8_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea540e-0bcc-43a4-a3af-c3d3d4e8eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime9_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set9_df, sigma_value, 100, target_emotional_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca61c0c0-0428-4280-8494-8f15d1c3a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime9_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8344e5-e9b4-41e9-a305-e3d62f092b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime10_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set10_df, sigma_value, 100, target_emotional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abaaacb-e413-4f48-a40e-80bd0a9ac562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime10_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb62f8-a094-4ac7-a191-5fcae1a3bd76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime11_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set11_df, sigma_value, 100, target_emotional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb81356-0ac6-4f49-8d17-8fcdac65f3f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime11_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c424fcd-99ee-41d2-9bf7-b3976884d7db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime12_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set12_df, sigma_value, 100, target_emotional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69747383-9e75-4264-91fd-17f78476aaa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime12_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61dd06b-f6f2-4536-9a5d-2a905861d158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime13_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set13_df, sigma_value, 100, target_emotional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e0611-03cd-4035-8eac-315ccf8e87dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_prime13_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875ef30-6ecc-46f6-85cc-343883e951d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime14_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set14_df, sigma_value, 100, target_emotional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc98e2-1446-47bd-b5d5-ac80a18cf8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime14_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ea0fa-9228-4629-81d9-e0fd3cfdd20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime15_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set15_df, sigma_value, 100, target_emotional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77516b49-8433-4756-94ea-d6f780b99b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime15_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a67a5f-56fc-4ebe-991b-53ad9aceee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime16_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set16_df, sigma_value, 100, target_emotional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa974e3c-faa5-44d1-a9ab-b5af20d85b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime16_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d8eb3-bd88-4e9a-9dbf-ac79d222f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime17_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set17_df, sigma_value, 100, target_emotional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a466e39e-b01a-486a-9ecc-25c31a727e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime17_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e40ad7-6ea3-4c1f-a1cb-6697b7152ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime18_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set18_df, sigma_value, 100, target_emotional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d5070f-a445-4cda-911b-c565d2a8dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime18_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8c9f8-7f0b-4ed1-aaff-485732b5d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime19_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set19_df, sigma_value, 100, target_emotional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763840cc-be69-491e-ad54-4d357b552751",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime19_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6aeeb5-fdd6-44d6-ab9c-c15cdb07ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_prime20_H2 = perform_causal_analysis_emotional_tokens(model_llama, tokenizer, hallu_set20_df, sigma_value, 100, target_emotional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d38a2-5255-4e7c-bb4b-77ac00210963",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime20_H2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformersenv)",
   "language": "python",
   "name": "transformersenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
